{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d496338-5545-424a-853e-4b1ebf1a654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b55ba3-a597-4cbf-8ca4-d1ab353bdd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab37a8ad-795e-4d70-8445-054d84b2cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef26a18c-5bbc-4e17-baf1-2b2d119ecb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from functools import partial\n",
    "from torch import einsum, nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import pack, rearrange, reduce, repeat, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c058093-9db1-478b-b65c-c69a891f10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllFile(base):\n",
    "    file_path = []\n",
    "    for root, ds, fs in os.walk(base, followlinks=True):\n",
    "        for f in fs:\n",
    "            fullname = os.path.join(root, f)\n",
    "            file_path.append(fullname)\n",
    "    return file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea8129-5c12-4bd7-84c2-7faa730d23af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc0cb8f2-80dc-4230-b8b5-9eefcd1b28ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780fadc5-8c82-494d-9062-e89c3d39fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "def load_vqvae(gen_cfg):\n",
    "\n",
    "    body_cfg = get_cfg_defaults()\n",
    "    body_cfg.merge_from_file(gen_cfg.vqvae.body_config)\n",
    "    body_model = (\n",
    "        instantiate_from_config(body_cfg.vqvae).to(device).eval()\n",
    "    )\n",
    "    body_model.load(os.path.join(body_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n",
    "    if gen_cfg.vqvae.left_hand_config is  None and gen_cfg.vqvae.right_hand_config is None:\n",
    "        return body_model, body_cfg\n",
    "    \n",
    "    if gen_cfg.vqvae.left_hand_config is not None:\n",
    "        left_cfg = get_cfg_defaults()\n",
    "        left_cfg.merge_from_file(gen_cfg.vqvae.left_hand_config)\n",
    "        left_hand_model = instantiate_from_config(left_cfg.vqvae).to(device).eval()\n",
    "        left_hand_model.load(\n",
    "            os.path.join(left_cfg.output_dir, \"vqvae_motion.pt\")\n",
    "        )\n",
    "    else:\n",
    "        left_hand_model = None\n",
    "        \n",
    "    if gen_cfg.vqvae.right_hand_config is not None:\n",
    "        right_cfg = get_cfg_defaults()\n",
    "        right_cfg.merge_from_file(gen_cfg.vqvae.right_hand_config)\n",
    "        right_hand_model = instantiate_from_config(right_cfg.vqvae).to(device).eval()\n",
    "        right_hand_model.load(\n",
    "            os.path.join(right_cfg.output_dir, \"vqvae_motion.pt\")\n",
    "        )\n",
    "    else:\n",
    "        right_hand_model = None\n",
    "\n",
    "    return body_model, left_hand_model , right_hand_model , body_cfg , left_cfg , right_cfg\n",
    "\n",
    "def bkn_to_motion( codes, dset , remove_translation = True):\n",
    "    # codes b k n\n",
    "\n",
    "    k = codes.shape[1]\n",
    "    mrep = dset.motion_rep\n",
    "\n",
    "    if k == 1:\n",
    "        if mrep == \"body\":\n",
    "\n",
    "            body_inds = codes[:, 0]\n",
    "            body_motion = body_model.decode(body_inds[0:1]).detach().cpu()\n",
    "\n",
    "            if remove_translation:\n",
    "                z = torch.zeros(\n",
    "                    body_motion.shape[:-1] + (2,),\n",
    "                    dtype=body_motion.dtype,\n",
    "                    device=body_motion.device,\n",
    "                )\n",
    "                body_motion = torch.cat(\n",
    "                    [body_motion[..., 0:1], z, body_motion[..., 1:]], -1\n",
    "                )\n",
    "\n",
    "            body_M = dset.toMotion(\n",
    "                body_motion[0],\n",
    "                motion_rep=MotionRep(\"body\"),\n",
    "                hml_rep=body_cfg.dataset.hml_rep,\n",
    "            )\n",
    "\n",
    "            return body_M\n",
    "\n",
    "        elif mrep == \"left_hand\":\n",
    "\n",
    "            left_inds = codes[:, 0]\n",
    "            left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "            left_M = dset.toMotion(\n",
    "                left_motion[0],\n",
    "                motion_rep=MotionRep(left_cfg.dataset.motion_rep),\n",
    "                hml_rep=left_cfg.dataset.hml_rep,\n",
    "            )\n",
    "            return left_M\n",
    "\n",
    "        elif mrep == \"right_hand\":\n",
    "            right_inds = codes[:, 0]\n",
    "            right_motion = (\n",
    "                right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "            )\n",
    "            right_M = dset.toMotion(\n",
    "                right_motion[0],\n",
    "                motion_rep=MotionRep(right_cfg.dataset.motion_rep),\n",
    "                hml_rep=right_cfg.dataset.hml_rep,\n",
    "            )\n",
    "            return right_M\n",
    "\n",
    "    if k == 2:\n",
    "        left_inds = codes[:, 0]\n",
    "        right_inds = codes[:, 1]\n",
    "\n",
    "        left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "        right_motion = right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "\n",
    "        left_M = dset.toMotion(\n",
    "            left_motion[0],\n",
    "            motion_rep=MotionRep(left_cfg.dataset.motion_rep),\n",
    "            hml_rep=left_cfg.dataset.hml_rep,\n",
    "        )\n",
    "        right_M = dset.toMotion(\n",
    "            right_motion[0],\n",
    "            motion_rep=MotionRep(right_cfg.dataset.motion_rep),\n",
    "            hml_rep=right_cfg.dataset.hml_rep,\n",
    "        )\n",
    "        hand_M = left_M + right_M\n",
    "        hand_M.motion_rep = MotionRep.HAND\n",
    "        hand_M.hml_rep = \"\".join(\n",
    "            [i for i in left_M.hml_rep if i in right_M.hml_rep]\n",
    "        )\n",
    "        return hand_M\n",
    "\n",
    "    if k == 3:\n",
    "        left_inds = codes[:, 1]\n",
    "        right_inds = codes[:, 2]\n",
    "        body_inds = codes[:, 0]\n",
    "        body_motion = body_model.decode(body_inds[0:1]).detach().cpu()\n",
    "\n",
    "        \n",
    "        if remove_translation:\n",
    "            z = torch.zeros(\n",
    "                body_motion.shape[:-1] + (2,),\n",
    "                dtype=body_motion.dtype,\n",
    "                device=body_motion.device,\n",
    "            )\n",
    "            body_motion = torch.cat([body_motion[..., 0:1], z, body_motion[..., 1:]], -1)\n",
    "\n",
    "        left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "        right_motion = right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "\n",
    "        body_M = dset.toMotion(\n",
    "            body_motion[0],\n",
    "            motion_rep=MotionRep(\"body\"),\n",
    "            hml_rep = body_cfg.dataset.hml_rep)\n",
    "\n",
    "        left_M = dset.toMotion(\n",
    "            left_motion[0],\n",
    "            motion_rep=MotionRep(\"left_hand\"),\n",
    "            hml_rep=left_cfg.dataset.hml_rep)\n",
    "        right_M = dset.toMotion(\n",
    "            right_motion[0],\n",
    "            motion_rep=MotionRep(\"right_hand\"),\n",
    "            hml_rep=right_cfg.dataset.hml_rep)\n",
    "        full_M = dset.to_full_joint_representation(body_M, left_M, right_M)\n",
    "        return full_M\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a13e71-76ff-4961-ba80-9234616627ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42b4c938-6eaf-487c-a25b-22b27b40aba2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9be613-4a23-4818-ae56-0e6051351065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd58688-785f-49b6-a1df-1a08026a59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config_t2o import get_cfg_defaults as trans_get_cfg_defaults\n",
    "trans_cfg = trans_get_cfg_defaults()\n",
    "trans_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/simple_motion_translation/simple_motion_translation.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24448e2-a514-4557-a785-a55d5ae2dd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e825e8-8240-49c4-9c06-bfa297a993be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n",
      "loaded model with  0.06821402907371521 tensor([40000.], device='cuda:0') steps\n"
     ]
    }
   ],
   "source": [
    "trans_model = instantiate_from_config(trans_cfg.vqvae).to(device).eval()\n",
    "trans_model.load(os.path.join(trans_cfg.output_dir, \"tcn_model.pt\"))\n",
    "trans_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "591f1346-d0a4-4bb0-9095-e887553721c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data: np.ndarray , data_root = \"/srv/hays-lab/scratch/sanisetty3/motionx\") -> np.ndarray:\n",
    "    mean_pos = torch.Tensor(np.load(os.path.join(data_root, \"motion_data/Mean_rel_pos.npy\"))[[0,2]]).to(data.device)\n",
    "    std_pos = torch.Tensor(np.load(os.path.join(data_root, \"motion_data/Std_rel_pos.npy\"))[[0,2]]).to(data.device)\n",
    "    return (data - mean_pos) / (std_pos + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061df894-bf46-4960-8bc6-eec64754a8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "787a59e9-2ce0-495b-be0e-277691e55447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "60502567-b4c3-4d43-8b95-340a0ba4ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj2orient(traj):\n",
    "    rel_pos = torch.zeros_like(traj)\n",
    "    rel_pos[..., 1:] = traj[..., 1:] - traj[..., :-1]\n",
    "    rel_pos2 = transform(rel_pos)\n",
    "    with torch.no_grad():\n",
    "        pred_orient = trans_model.predict(rel_pos2.to(device))\n",
    "\n",
    "    return pred_orient, rel_pos\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68667620-a494-4eb9-a702-aa5804ec4312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc7dd5d1-0dca-43e9-a16f-c981ce5260f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c8e17f8-e2f3-44af-b987-bba70089e439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_orient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c3539b7-9b04-4203-965b-0b96ec993d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_recover_root_rot_pos(r_rot_quat, r_pos):\n",
    "    ## B N 4, B N 2\n",
    "    # Step 1: Compute the original root linear velocity\n",
    "    root_linear_velocity = torch.zeros_like(r_pos[..., :, [0, 1]])\n",
    "    root_linear_velocity[..., 1:, 0] = r_pos[..., 1:, 0] - r_pos[..., :-1, 0]\n",
    "    root_linear_velocity[..., 1:, 1] = r_pos[..., 1:, 1] - r_pos[..., :-1, 1]\n",
    "\n",
    "    # Step 2: Compute the original root rotation velocity\n",
    "    r_rot_ang = torch.atan2(r_rot_quat[..., 2], r_rot_quat[..., 0])\n",
    "    root_rot_velocity = torch.zeros_like(r_rot_ang)\n",
    "    root_rot_velocity[..., 1:] = r_rot_ang[..., 1:] - r_rot_ang[..., :-1]\n",
    "\n",
    "    # Step 3: Combine root linear velocity and root rotation velocity to get root_params\n",
    "    # root_params = torch.cat((root_rot_velocity.unsqueeze(-1), root_linear_velocity, r_pos[..., 1:, [1]].unsqueeze(-1)), dim=-1)\n",
    "\n",
    "    root_params = torch.cat((root_rot_velocity.unsqueeze(-1), root_linear_velocity), dim=-1)\n",
    "\n",
    "\n",
    "    return root_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4616c06-2354-45d7-887c-53ac83412463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67628-5307-4784-ba99-d310f3427943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39a12a26-919f-43a3-99d3-85fe85238c79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e0f0addc-a1c0-41c0-be21-4979791324be",
   "metadata": {},
   "outputs": [],
   "source": [
    "refiner_cfg = get_cfg_defaults()\n",
    "refiner_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_full_gpvc/vqvae_full_gpvc.yaml\")\n",
    "\n",
    "# dataset_args = refiner_cfg.dataset\n",
    "refiner_model = instantiate_from_config(refiner_cfg.vqvae).to(device).eval()\n",
    "# refiner_model.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body_gprvc/vqvae_motion.pt\")\n",
    "refiner_model.load(os.path.join(refiner_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "refiner_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d30fe-9608-430f-ac7d-b1b42d3b2b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ef0c8-0d91-4564-bd19-483988dcf67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccfb81-3746-43b1-8ffd-592b72fd4bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34daa9-f416-4565-a7d0-f25b90a479dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3575f420-dce1-4154-a10e-bb91d8f42551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8142350-60d7-42d7-abdb-9c8b584f6366",
   "metadata": {},
   "source": [
    "## MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95e47c32-ea9b-47dc-9b56-07f8e1047def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.generation.muse2 import generate_animation\n",
    "from core import MotionTokenizerParams, pattern_providers\n",
    "\n",
    "from core.param_dataclasses import pattern_providers\n",
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n",
    "from core import MotionRep, AudioRep, TextRep\n",
    "from core.datasets.conditioner import ConditionProvider,ConditionFuser\n",
    "from core.models.generation.muse2 import MotionMuse as MotionMuse2\n",
    "import einops\n",
    "from configs.config_t2m import get_cfg_defaults as muse_get_cfg_defaults\n",
    "from core import MotionTokenizerParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14d2fa13-4281-4b71-895c-eeca6dca9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = muse_get_cfg_defaults()\n",
    "gen_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_muse_body_hands/motion_muse_body_hands.yaml\")\n",
    "gen_cfg.freeze()\n",
    "tranformer_config = gen_cfg.motion_generator\n",
    "fuse_config = gen_cfg.fuser\n",
    "pattern_config = gen_cfg.codebooks_pattern\n",
    "dataset_args = gen_cfg.dataset\n",
    "\n",
    "target = tranformer_config.pop(\"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15bd1abf-72c1-47e8-80b8-8c7e858d76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_gen = MotionMuse2(tranformer_config , fuse_config , pattern_config).to(device).eval()\n",
    "pkg = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_muse_body_hands/motion_muse.pt\", map_location=\"cuda\")\n",
    "motion_gen.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "185a790b-a605-4c33-8566-186fa1ebaba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n",
      "loaded model with  0.03015906736254692 tensor([110000.], device='cuda:0') steps\n"
     ]
    }
   ],
   "source": [
    "body_model, left_hand_model , right_hand_model , body_cfg , left_cfg , right_cfg = load_vqvae(gen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b00cf0e6-e12b-4c2d-95dd-8487663edeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            text_conditioner_name = dataset_args.text_conditioner_name,\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            audio_padding=dataset_args.audio_padding,\n",
    "            motion_max_length_s=dataset_args.motion_max_length_s,\n",
    "            audio_max_length_s=dataset_args.audio_max_length_s,\n",
    "            pad_id = MotionTokenizerParams(tranformer_config.num_tokens).pad_token_id,\n",
    "            fps=30/4,\n",
    "            # device = \"cpu\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "661d6fd2-9012-4f62-9447-501d9884d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.base_dataset import BaseMotionDataset\n",
    "base_dset = BaseMotionDataset(motion_rep=MotionRep.BODY , hml_rep= \"gpvc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891512f-a013-4df8-9769-49caa71de388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900b49e-1771-4fb0-b682-32c8105b4880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "946bda68-e311-4734-a7e3-bd94a7c1992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_clip =  None #\"/srv/hays-lab/scratch/sanisetty3/motionx/audio/wav/wild/despacito.mp3\"\n",
    "text_ = \"A person walking forward\"\n",
    "neg_text_ = None, #\"dancing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb8ad967-27c1-4d83-b820-30897abd71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 43.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 43.95it/s]\n"
     ]
    }
   ],
   "source": [
    "all_ids_body = generate_animation(motion_gen , condition_provider ,overlap = 5, duration_s = 8 , aud_file=aud_clip, text = text_ , neg_text=neg_text_, use_token_critic = True, timesteps = 24 )\n",
    "gen_motion = bkn_to_motion(all_ids_body, base_dset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a2e940f-2124-4763-8f3f-d6e7e76ac24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([220, 317])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_motion().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1df6c54-ada6-4976-8e76-4fe8801a56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dset.render_hml(\n",
    "                    gen_motion,\n",
    "                    f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gen_novel_full.gif\",\n",
    "                    zero_trans = True,\n",
    "                    zero_orient = True,\n",
    "    \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba607745-7bac-4fc2-963b-344636873681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65072b5-4088-48e6-bf21-197b858dbb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc9f32-c618-4fc6-b236-dbadcac011ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90d4d76e-0558-428f-a6d2-1ed474c5ec85",
   "metadata": {},
   "source": [
    "## Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dad41273-0a9b-4a05-b14e-b6c943172bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj2orient(traj):\n",
    "    rel_pos = torch.zeros_like(traj)\n",
    "    rel_pos[..., 1:] = traj[..., 1:] - traj[..., :-1]\n",
    "    rel_pos2 = transform(rel_pos)\n",
    "    with torch.no_grad():\n",
    "        pred_orient = trans_model.predict(rel_pos2.to(device))\n",
    "\n",
    "    return pred_orient, rel_pos\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eae27fa5-3fe8-4346-a91f-025565d96294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 220]) torch.Size([1, 220, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rel_pos = torch.zeros_like(traj)\n",
    "rel_pos[..., 1:] = traj[..., 1:] - traj[..., :-1]\n",
    "pred_orient = traj2orient(traj)\n",
    "root_params_pred = reverse_recover_root_rot_pos(pred_orient , traj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "264077cd-5496-49d2-a639-9fa4aef3ba14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2693ff0e-e938-414a-8b45-02392a330826",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_orient, rel_pos= traj2orient(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a38aba-96a4-4652-a515-4df31267bf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45414b0-d78e-42fd-8411-d61123ada74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1a5227c8-57d6-4f78-9906-cc7457d4a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_motion = bkn_to_motion(all_ids_body, base_dset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d708d7-bffc-4e91-8b4e-46662aefc61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974d507-bfc3-4d88-a6a5-bc63f752b502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8713ff52-e6c1-49df-9a8b-1b763964ac49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6945da-d1d5-48e9-8ca0-e64b2135c050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bb14f-7254-48e1-8031-d950aaed0b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "561e58f0-55e2-4ce9-844e-ca964ccda44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = torch.zeros((1 , 220 , 2)).to(device)\n",
    "traj[...,1] = torch.linspace(0,20,220)\n",
    "rel_pos = torch.zeros_like(traj)\n",
    "rel_pos[:, 1:] = traj[:, 1:] - traj[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "926dad1a-b71d-4f3d-88e6-81143f89556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_motion = bkn_to_motion(all_ids_body, base_dset)\n",
    "ohpvc_full = gen_motion\n",
    "ohpvc_full_inv = base_dset.inv_transform(ohpvc_full)\n",
    "ohpvc_full_inv.root_params[:,1:3] = rel_pos\n",
    "ohpvc_full_inv.root_params[:,0:1] = 0\n",
    "ohpvc_full_trns = base_dset.transform(ohpvc_full_inv)\n",
    "ohpvc_full_inv.contact[:,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cad45-60da-46d0-b350-279d0f15b5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6eec7ff4-6820-484b-bc89-548e1be95140",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = refiner_model(ohpvc_full_trns()[None].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "09b81afb-75d9-414c-8569-d3ed0c71dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_Motion = base_dset.toMotion(out.decoded_motion[0] , MotionRep(\"full\") , hml_rep = \"gpvc\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7acd8-df41-4453-9cb6-beead2ed5dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9bc12b5d-5391-4759-9a68-4d0751811b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dset.render_hml(\n",
    "                    refined_Motion,\n",
    "                    f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gen_novel_full_add_traj.gif\",\n",
    "                    # zero_trans = True,\n",
    "                    # zero_orient = True,\n",
    "    \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8c65ee-dba0-4054-8148-b93bda81b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image(open(f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gen_novel_full_add_traj.gif\",'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6c960-54fd-4b50-8523-ba722227f320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
