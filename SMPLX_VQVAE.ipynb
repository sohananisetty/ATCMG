{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ddc593-335a-4c95-925d-a1aef161033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if device.type == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Memory Usage:\")\n",
    "    print(\"Allocated:\", round(torch.cuda.memory_allocated(0) / 1024**3, 1), \"GB\")\n",
    "    print(\"Cached:   \", round(torch.cuda.memory_reserved(0) / 1024**3, 1), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c473c419-8c66-4f10-9467-f1f15b5bb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9785cf10-ca24-45fe-b6d7-726c1bf59178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "# from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843bcc6-d85b-4286-b267-eca5d4c06502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f95e286-bc5c-4f51-9416-080935ab6087",
   "metadata": {},
   "source": [
    "## test rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ed2250-2a08-4fec-beb3-2738486b89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllFile(base):\n",
    "    \"\"\"\n",
    "    Recursively find all files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        base (str): The base directory to start the search.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths found in the directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    file_path = []\n",
    "    for root, ds, fs in os.walk(base, followlinks=True):\n",
    "        for f in fs:\n",
    "            fullname = os.path.join(root, f)\n",
    "            file_path.append(fullname)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b268b5d7-502f-428f-899b-f2470587299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_processing.hml_process import recover_from_ric, recover_root_rot_pos\n",
    "import utils.vis_utils.plot_3d_global as plot_3d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f856a3f-bd13-492a-9f13-402643349384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from core.param_dataclasses import pattern_providers\n",
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n",
    "from core import MotionRep, AudioRep, TextRep\n",
    "from core.datasets.conditioner import ConditionProvider,ConditionFuser\n",
    "from core.models.generation.lm import LMModel, MotionGen\n",
    "\n",
    "from configs.config import cfg, get_cfg_defaults\n",
    "from configs.config_streaming import get_cfg_defaults as strm_get_cfg_defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51578303-4e64-4ecc-b605-36868fe00e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4879bf7-3898-41f6-84ae-92ca96477033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9701a6-f501-43e2-b93b-db96d8a1f016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fe43b-a893-429a-9c89-56bf15b2293e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701ef95c-1211-4f80-82d1-2c59aaf4db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = strm_get_cfg_defaults()\n",
    "gen_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_streaming/motion_streaming.yaml\")\n",
    "gen_cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8daa247-ea5b-45f5-9a71-39026d265693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "body_cfg = get_cfg_defaults()\n",
    "body_cfg.merge_from_file(gen_cfg.vqvae.body_config)\n",
    "left_cfg = get_cfg_defaults()\n",
    "left_cfg.merge_from_file(gen_cfg.vqvae.left_hand_config)\n",
    "right_cfg = get_cfg_defaults()\n",
    "right_cfg.merge_from_file(gen_cfg.vqvae.right_hand_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7ed910-46ab-4f39-94d7-13c4e525a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.resnetVQ.vqvae import HumanVQVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87359b5-80b1-45a6-afcd-2bff0f3180a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ca6f05f-505b-42dc-a7c2-6c7a2309ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_hand_model = HumanVQVAE(left_cfg.vqvae).to(device).eval()\n",
    "left_hand_model.load(os.path.join(left_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n",
    "right_hand_model = HumanVQVAE(right_cfg.vqvae).to(device).eval()\n",
    "right_hand_model.load(os.path.join(right_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n",
    "body_model = HumanVQVAE(body_cfg.vqvae).to(device).eval()\n",
    "body_model.load(os.path.join(body_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f8407-7045-4cd3-a0c9-a4ebae3ca54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a36f7a2-c271-4998-9647-d89951a2bbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54003cfb-544e-419b-b143-315c7a832876",
   "metadata": {},
   "source": [
    "### VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf13d1-9fe5-4a0b-9f48-039763e9a6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2803dc9a-5276-4e3f-bfef-a12c919b6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vq_dataset import VQSMPLXMotionDataset\n",
    "from core.datasets.vq_dataset import load_dataset as load_dataset_vq\n",
    "from core.datasets.vq_dataset import simple_collate as simple_collate_vq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9c5eeeb-5648-4e6a-9fc9-000d083d2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoded(model , motion):\n",
    "    encs = []\n",
    "    inds = []\n",
    "    for i in range(0 , motion.shape[1] , 120 ):\n",
    "        if i + 240 >= motion.shape[1]:\n",
    "            enc_b = model(motion[: , i:, :].to(device))\n",
    "            encs.append(enc_b.decoded_motion)\n",
    "            inds.append(enc_b.indices)\n",
    "            break\n",
    "        else:\n",
    "            enc_b = model(motion[: , i:i + 120, :].to(device))\n",
    "            encs.append(enc_b.decoded_motion)\n",
    "            inds.append(enc_b.indices)\n",
    "    return torch.cat(encs , 1), torch.cat(inds , -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6bfa9eb6-bebd-44b2-aae6-f14335230ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "condition_provider2 = ConditionProvider(\n",
    "            motion_rep=MotionRep(\"full\"),\n",
    "            motion_padding=\"max_length\",\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "359ff182-863e-4745-b0fe-a1aa11b3cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_cfg.dataset.window_size = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4a7fd60c-c052-4e26-a206-7d2101f75c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 312\n",
      "Total number of motions humanml: 19984\n",
      "Total number of motions perform: 451\n",
      "Total number of motions GRAB: 1268\n",
      "Total number of motions idea400: 11886\n",
      "Total number of motions humman: 706\n",
      "Total number of motions beat: 1458\n",
      "Total number of motions game_motion: 9699\n",
      "Total number of motions music: 3385\n",
      "Total number of motions aist: 1396\n",
      "Total number of motions fitness: 15886\n",
      "Total number of motions moyo: 161\n",
      "Total number of motions choreomaster: 34\n",
      "Total number of motions dance: 153\n",
      "Total number of motions kungfu: 987\n",
      "Total number of motions EgoBody: 931\n",
      "Total number of motions HAA500: 4969\n"
     ]
    }
   ],
   "source": [
    "test_ds, _, _ = load_dataset_vq(\n",
    "            # dataset_names=[\"choreomaster\"],\n",
    "            dataset_args=body_cfg.dataset,\n",
    "            split=\"train\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5744bd23-da74-47e0-91e9-3f4406d6301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(\n",
    "            test_ds,\n",
    "            batch_size=1,\n",
    "            sampler=None,\n",
    "            shuffle=False,\n",
    "            collate_fn=partial(simple_collate_vq , conditioner = condition_provider2),\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdbd76-0735-4505-b246-2efbdea7a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▍                                                      | 22110/73666 [3:12:26<8:07:42,  1.76it/s]"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for batch in tqdm(dl):\n",
    "    sve = os.path.join(\"/srv/hays-lab/scratch/sanisetty3/motionx/indices/body_rv\" , batch[\"names\"][0] )\n",
    "    if os.path.exists(sve):\n",
    "        continue\n",
    "    os.makedirs(os.path.dirname(sve) , exist_ok=True)\n",
    "    try:\n",
    "        enc_b, inb = get_decoded(body_model , inputs[\"motion\"][0].to(device))\n",
    "        np.save(sve , inb.cpu().numpy())\n",
    "    except:\n",
    "        errors.append(batch[\"names\"][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fe123c5-8890-4255-b5e5-ecbb30c9c202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1a3096d-bf35-49da-a1fd-a6eaf518e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6466, 192)\n",
      "(1, 6466, 192)\n"
     ]
    }
   ],
   "source": [
    "for inputs in dl:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aae7d1d3-91a1-4c12-b451-c59f7444aa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6466, 192])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"motion\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdc2c9-c7b8-459d-b661-8963db2769fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2412f69d-56fa-4a48-a285-4369a8d4a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = body_model.encode(inputs[\"motion\"][0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2fada69-3155-416e-bb8a-1ce7740ff288",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = body_model.decode(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7f348b8e-3494-48a6-bd26-33d746704a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6464, 192])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29e5d592-4fc9-4c63-b0ba-a7fb56c7d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2, cde2 = get_decoded(body_model , inputs[\"motion\"][0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23997e59-e014-4f44-b4c3-bd7a5357f199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea2b6ba6-ae01-461b-961f-922d6206a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.datasets[0].render_hml(\n",
    "                    out2.detach().squeeze().cpu()[:500],\n",
    "                    \"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/choreo2.gif\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0889acc0-492a-4543-9d60-0a27e9e2b1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['choreomaster/1160.npy'], dtype='<U21')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f45a8b-babb-4200-a9e6-88047ebd5409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a020206-3c85-4ce9-9dc1-d0ca0302a588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:19<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a62b4-818f-473c-aa42-cdbec2b5b707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625a9ae-8084-4b02-b46e-6f108bcf880e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58315bf7-963e-495e-beff-f5477164a0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d129356-434a-45d1-823a-631c4d3845cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f83ed-3824-48ff-b107-43aabb198d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f92dc8-0bd8-4b5b-9e6b-9590289b4a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ca08b-ad66-431d-83eb-96bd895dac49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23af9bfe-0533-471d-bbb6-3bf76003ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_args = gen_cfg.transformer_lm\n",
    "target = lm_args.pop(\"target\")\n",
    "fuse_config = gen_cfg.fuser\n",
    "pattern_args = gen_cfg.codebooks_pattern\n",
    "dataset_args = gen_cfg.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96181eba-25b4-46f7-92b3-6af9d71b5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gen= MotionGen(lm_args , fuse_config , pattern_args ).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde4147-18cb-47b3-b9a1-f68333e80712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19531d79-28aa-4ca0-a874-36cff496f26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            audio_padding=dataset_args.audio_padding,\n",
    "            motion_max_length_s=10,\n",
    "            audio_max_length_s=10,\n",
    "            pad_id = model_gen.model.pad_token_id,\n",
    "            fps=30/4,\n",
    "            # device = \"cpu\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58d170cd-bc16-4d0d-9971-a55deb18a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "# dset = MotionIndicesAudioTextDataset(\"chroeomaster\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"full\", split = \"train\" , fps = 30/4  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f5a58db-0967-4985-9d28-e3b69e7d6d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 194 and texts 194\n",
      "Total number of motions choreomaster: 34 and texts 34\n"
     ]
    }
   ],
   "source": [
    "train_ds, sampler_train, weights_train  = load_dataset_gen(dataset_args=dataset_args, split = \"train\" , dataset_names = [\"animation\" , \"choreomaster\" ] )\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        4,\n",
    "        sampler=sampler_train,\n",
    "        # shuffle = False,\n",
    "        collate_fn=partial(simple_collate , conditioner = condition_provider , permute = True),\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4297e50e-0e32-4bef-8720-04dc0f06680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, conditions in train_loader:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e702c8bd-2a6d-4178-93cf-41c05f80f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask = inputs[\"motion\"][1]\n",
    "motions_or_ids = inputs[\"motion\"][0]\n",
    "B, K, T = motions_or_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea14f0-3280-4331-a467-02b72f4f27f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e57bd-3bb3-4ba4-b7aa-aa83724b36e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaedbf-4bee-475b-8c4b-6c874e9f97a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
