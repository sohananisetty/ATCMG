{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231a3e8-9e24-4990-b940-00cdc6b15e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c44eef0-cea4-4ac9-b924-1327280f235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d81025f-b2fd-4067-9c7a-5c21f74f8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d36d72-1eba-4fd0-ba1e-69a4e714339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from functools import partial\n",
    "from torch import einsum, nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70de7dab-5b08-4278-872f-ca3885ac3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllFile(base):\n",
    "    file_path = []\n",
    "    for root, ds, fs in os.walk(base, followlinks=True):\n",
    "        for f in fs:\n",
    "            fullname = os.path.join(root, f)\n",
    "            file_path.append(fullname)\n",
    "    return file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395afd96-a4f7-45e3-b006-57abc3191800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.motion_processing.hml_process import recover_from_ric, recover_root_rot_pos,recover_from_rot\n",
    "import utils.vis_utils.plot_3d_global as plot_3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def vis(mot , dset , name = \"motion\"):\n",
    "\n",
    "    if isinstance(mot , torch.Tensor):\n",
    "        mot = dset.toMotion(mot)\n",
    "    mot =dset.inv_transform(mot)\n",
    "\n",
    "\n",
    "\n",
    "    xyz = np.array(dset.to_xyz(mot).cpu())\n",
    "\n",
    "    print(xyz.shape)\n",
    "\n",
    "    \n",
    "    plot_3d.render(xyz , f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/{name}.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f8324-7df7-471e-9832-56b3e3c4c9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1523299e-4f73-48b0-8e16-03e03464321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body/vqvae_body.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538bd7db-2d03-44e0-a757-98be718d734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.conditioner import ConditionProvider, ConditionFuser\n",
    "from core.datasets.multimodal_dataset import MotionAudioTextDataset, load_dataset, simple_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbffeb7-ce5f-4ee5-8c9f-280090d0729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.attend import Attention\n",
    "from core import AttentionParams\n",
    "from core import AttentionParams, TranslationTransformerParams, PositionalEmbeddingParams, PositionalEmbeddingType, MotionRep, AudioRep, TextRep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49f3ff-34de-4ab9-91d2-d7fa22f60310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c01eae-afd4-40a0-93d5-e157c3f081c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d84de22-8d50-499f-b8c9-4b36fc54b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.conditioner import ConditionProvider, ConditionFuser\n",
    "from core.datasets.multimodal_dataset import MotionAudioTextDataset, load_dataset, simple_collate\n",
    "dataset_args = cfg.dataset\n",
    "\n",
    "\n",
    "condition_provider2 = ConditionProvider(\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a6c40a-46d6-4224-8264-728c16e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.multimodal_dataset import MotionAudioTextDataset\n",
    "from core.datasets.vq_dataset import VQSMPLXMotionDataset\n",
    "from core.datasets.vq_dataset import load_dataset, simple_collate as simple_collate2\n",
    "from core import Motion\n",
    "\n",
    "from utils.motion_processing.skeleton import Skeleton, t2m_kinematic_chain , body_joints_id, t2m_raw_body_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db03f747-24d7-4ef0-81ed-27b999193a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = MotionAudioTextDataset(\"moyo\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"body\" , hml_rep = \"gprvc\", split = \"test\"   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "88487a56-d2bf-4be8-a0d3-caa773a012db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = VQSMPLXMotionDataset(\"choreomaster\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"body\" , hml_rep = \"rv\", split = \"train\" , window_size = 600  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37ff16e6-0b76-40f3-a621-22c48e3163ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 2\n",
      "Total number of motions humanml: 2944\n",
      "Total number of motions perform: 16\n",
      "Total number of motions GRAB: 67\n",
      "Total number of motions idea400: 577\n",
      "Total number of motions humman: 19\n",
      "Total number of motions beat: 162\n",
      "Total number of motions game_motion: 130\n",
      "Total number of motions music: 148\n",
      "Total number of motions aist: 61\n",
      "Total number of motions fitness: 572\n",
      "Total number of motions moyo: 9\n",
      "Total number of motions choreomaster: 2\n",
      "Total number of motions dance: 7\n",
      "Total number of motions kungfu: 42\n",
      "Total number of motions EgoBody: 49\n",
      "Total number of motions HAA500: 17\n"
     ]
    }
   ],
   "source": [
    "test_ds, _, _ = load_dataset(\n",
    "            dataset_args=dataset_args,\n",
    "            split=\"test\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a913b-15de-48f4-8119-a4bd11148db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e823bc41-08e3-4b4d-ae60-923d5e00c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        test_ds,\n",
    "        100,\n",
    "        # sampler=sampler,\n",
    "        collate_fn=partial(simple_collate2 , conditioner = condition_provider2),\n",
    "        # drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd5510e5-d665-4f72-880c-366803188456",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs in train_loader:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c27946fe-185d-4751-9c5e-78b571d0579d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 120, 263])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot = inputs[\"motion\"][0]\n",
    "mot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f71a6d8-ea08-4318-8cef-30bbf340e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.render_hml(mot[0] , \"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/r.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983f84f-040c-44a7-8766-e0c06aa3636d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e8323-0a3a-462f-a7ad-22116128b288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9328e5dd-ac7b-432b-91b1-2ebe4a9d4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.resnetVQ.vqvae import HumanVQVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74993259-b07b-4f4d-a975-025467755ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vqvae_args = cfg.vqvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb567ed-0fdb-4ab4-a87f-4148770c737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_args.nb_joints = test_ds.datasets[0].nb_joints\n",
    "vqvae_args.motion_dim = test_ds.datasets[0].motion_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385245a-2583-432d-9766-35a3c6244f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c05c20d6-13b2-486c-8c91-9642fce9b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = HumanVQVAE(vqvae_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59a33e07-e8ef-4c26-8f81-72d93ccf68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body/checkpoints/vqvae_motion.110000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518cf62-f15d-4f50-bc14-6db53ea053ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35e540-5800-4c71-b188-6191cc8b5105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e99254-2b8b-4a47-a244-3ebf4af4a445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939df00-8e8e-4e2a-b4de-28e8d3c57859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee4d92a8-8311-4634-8c11-780f8f14b25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 49/49 [00:07<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vqvae_model.eval()\n",
    "val_loss_ae = {}\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(\n",
    "        (train_loader),\n",
    "        position=0,\n",
    "        leave=True,\n",
    "    ):\n",
    "        gt_motion = batch[\"motion\"][0].to(device)\n",
    "\n",
    "        indices = vqvae_model.encode(\n",
    "            motion=gt_motion,\n",
    "            # mask=mask,\n",
    "        )\n",
    "\n",
    "        # print(gt_motion.shape[0])\n",
    "\n",
    "        # loss_motion = loss_fnc(\n",
    "        #     vqvae_output.decoded_motion, gt_motion, mask=None\n",
    "        # )\n",
    "\n",
    "        # loss = (\n",
    "        #     vqvae_args.loss_motion * loss_motion\n",
    "        #     + vqvae_args.commit * vqvae_output.commit_loss\n",
    "        # ) \n",
    "\n",
    "        used_indices = indices.flatten().tolist()\n",
    "        usage = len(set(used_indices)) / vqvae_args.codebook_size\n",
    "        # print(usage)\n",
    "\n",
    "        loss_dict = {\n",
    "            # \"total_loss\": loss.detach().cpu(),\n",
    "            # \"loss_motion\": loss_motion.detach().cpu(),\n",
    "            # \"commit_loss\": vqvae_output.commit_loss.detach().cpu(),\n",
    "            \"usage\": usage,\n",
    "        }\n",
    "\n",
    "        cnt+=1\n",
    "\n",
    "        for key, value in loss_dict.items():\n",
    "            if key in val_loss_ae:\n",
    "                val_loss_ae[key] += value\n",
    "            else:\n",
    "                val_loss_ae[key] = value\n",
    "\n",
    "\n",
    "for key in val_loss_ae.keys():\n",
    "    val_loss_ae[key] = val_loss_ae[key] / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ad3774-0bf4-43b6-aba2-a2b8ac428a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usage': 0.5708107461734694}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0b972-0c0a-4dd2-9172-81328aea0168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e09d6-84ef-49b1-897c-b4bba49a67d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fee5c-7791-414d-83bf-53cfafa01b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd5015-678b-417d-8d4a-4caa9a332754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be98079-2a1a-450c-a83c-5b369aa27804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d177822-be83-407c-b878-54f49a06028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "727075fa-2e54-4210-bcfa-1aff2e8b5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.loss import ReConsLoss\n",
    "\n",
    "loss_fnc = ReConsLoss(\"l1_smooth\" , True , vqvae_args.nb_joints , hml_rep=dset.hml_rep , motion_rep = dset.motion_rep  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134c0ac-4415-4764-b0f4-4356dfb2ff47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27936f3d-6689-4a80-b86a-8d009d101b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = inputs[\"motion\"][0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb389fa3-0cfb-4d91-935c-3af122a45568",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = vqvae_model(motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4bca189-be1c-41c4-bc5e-96f6930d36a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 192])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.decoded_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ff2b2ad-6d96-4c29-8ee2-dd8ecfb57cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fnc(pred.decoded_motion , motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486398a-5a6c-4a0a-a50f-1842abf4892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de676aac-a82e-4380-83d1-592d4022d303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7590eb-21af-415a-9e3e-da25820ea7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b01a49-d07c-4f10-884a-f9a40e900425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12a89bfa-687b-44dd-b498-a43e0ef7b816",
   "metadata": {},
   "source": [
    "### Motion MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dad8049-a4a0-4612-97ae-06e6e22ffb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config_t2m import cfg, get_cfg_defaults\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_generation/motion_generation.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5fb8d3b-a01c-4d29-8c2b-635e1daf80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import MotionRep, AudioRep, TextRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510ab9d2-a089-48ef-b155-7b58f558ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.conditioner import ConditionProvider, ConditionFuser\n",
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset, simple_collate\n",
    "from core.models.generation.motion_generator import Transformer, MotionMuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1f7b4-6181-4bbf-be86-53f21f954486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646cf7c-5536-43f8-98dc-335c8791ac23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11b3741-b00d-436a-8c92-c5e7f307eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmuse_args = cfg.motion_generator\n",
    "dataset_args = cfg.dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a610ad-b8b0-473f-a68d-89f059769358",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmuse_args.flash = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f9f328-cc22-4965-a1b1-bc1aa86f33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = mmuse_args.pop(\"target\")\n",
    "no_mask_token_prob =  mmuse_args.pop(\"no_mask_token_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d760c-4e05-4a9b-880f-c43247e0c128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201034b-55fe-4bd1-abea-5c0ace8c38e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10118dea-3725-4966-8479-b1c7fe9b002e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6ec7ff8-2137-4926-852b-a9cd94d5f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Transformer(**mmuse_args).to(device)\n",
    "motion_muse = MotionMuse(transformer = trans , no_mask_token_prob = no_mask_token_prob ).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31e4d76d-6cc5-4fe2-bd4b-7b5984f591eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            audio_padding=dataset_args.audio_padding,\n",
    "            motion_max_length_s=10,\n",
    "            audio_max_length_s=10,\n",
    "            pad_id = trans.pad_token_id,\n",
    "            fps=30/4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e44106b1-f9a2-45cb-9a2e-e20773e110f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 17 and texts 17\n"
     ]
    }
   ],
   "source": [
    "dset = MotionIndicesAudioTextDataset(\"animation\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"body\", split = \"test\" , fps = 30/4  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29b1d3c1-2084-4c57-bd85-b391f57e418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dset,\n",
    "        4,\n",
    "        # sampler=sampler,\n",
    "        collate_fn=partial(simple_collate , conditioner = condition_provider),\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce18ef6b-8c40-4b3e-b4b4-704d756b863d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for inputs, conditions in train_loader:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29e105-0941-4065-bce0-caad77431ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c96e9f-76a2-4789-a1a8-fd1884160438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d919f-648e-41e3-ae8e-e8497cd919ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfa115-2e35-47c0-a7af-2ff6753a1a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "393ce2ec-6c67-43c4-b7cc-1c2478d654ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 28])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1e85eb3-23fc-4172-8d24-b8376f50c396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bool"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_mask.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb4cb22e-4566-4f10-95a4-d3b6e62d3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "motions = inputs[\"motion\"][0].squeeze().to(torch.long)\n",
    "motion_mask = inputs[\"motion\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12cc9378-24d1-4a80-8127-0f37701e8195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = motion_muse((motions , motion_mask) , conditions , cond_drop_prob = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4446cd2-9d83-4c0a-91fb-308d3a1fe43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits =motion_muse.transformer.forward_with_cond_scale((motions , motion_mask) , conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abdc0dd2-6fd3-4ee1-9c31-64d26aa47fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits =motion_muse.transformer.forward_with_neg_prompt((motions , motion_mask) , conditions , conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8823b0-c190-40b4-b8db-9af144202f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "941f570a-d0d2-4fca-8631-abb6f1e97084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 18/18 [00:01<00:00, 15.90it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_ids = motion_muse.generate(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2fad7f38-68a5-470f-be70-49271deb6e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[297, 297, 297,  ..., 297, 297, 297],\n",
       "        [297, 297, 297,  ..., 297, 297, 297],\n",
       "        [297, 297, 297,  ..., 297, 297, 297],\n",
       "        [297, 297, 297,  ..., 297, 297, 297]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba7b102f-d5b4-488e-9896-fa208cd203cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[\"text\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65e0a8-6f4d-467f-ae3f-5ebabf0bbbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64e7eaf4-8d3e-44d0-a0c9-6809baa88a06",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301ed47-259c-4850-a3b0-370b41eab8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"a man dancing\"\n",
    "audio = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45e2f84f-c52a-475c-8614-fccf5bd609da",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud , am = condition_provider._get_audio_features(audio_list = [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7505174-0e14-4765-a886-e6b2de615524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 128)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72854d41-ae2e-4b22-9967-479e5151b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc74f7-78e4-4401-880d-7cb73ba151b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d5ff5-310c-4604-bdbe-2baa537d08c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a08eab58-a9aa-4b5f-8e8a-8f10b395752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sap = AttentionParams(dim = 256 , causal=True)\n",
    "cap = AttentionParams(dim = 256 , causal=True , add_null_kv=True)\n",
    "transformer_params = TranslationTransformerParams(self_attention_params = sap , \n",
    "                                                  cross_attention_params = cap , \n",
    "                                                  depth = 1, \n",
    "                                                  positional_embedding_params=PositionalEmbeddingParams(dim = 256) , \n",
    "                                                  positional_embedding=PositionalEmbeddingType.SINE,\n",
    "                                                  fuse_method = {\"cross_seperate\" : [\"audio\" , \"text\"]}\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630d974-93d0-4280-95bd-af6608b7c25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36579128-c825-4f7e-9416-b852d8536cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = transformer_blocks(\n",
    "            x=x_,\n",
    "            mask=x_padding_mask,\n",
    "            context=context,\n",
    "            context_mask=context_padding_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "557f7668-0ab2-47de-8f5b-3389a5c0751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embed[:, -n:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2e0c1b4-5cd3-494e-b89e-7cb40efd70fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 116, 256])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e4741-b259-4a50-a2ae-4ebf7ac1a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenCLIPEmbedder(nn.Module):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
    "    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77):\n",
    "        super().__init__()\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "        self.transformer = CLIPTextModel.from_pretrained(version)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.freeze()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.transformer = self.transformer.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
    "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "        outputs = self.transformer(input_ids=tokens)\n",
    "\n",
    "        z = outputs.last_hidden_state\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de48aa-bc99-4d66-921d-da96baa01e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38264d9e-7b0e-450c-a8fd-45464f90a4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16676f3f-c4e0-402e-8192-79b24528c866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f85cfd87-d174-419a-81d7-895447c01313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb = ScaledSinusoidalEmbedding(PositionalEmbeddingParams(dim = 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "983afa5c-2dc2-4733-8081-3e7e644bd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b , n , _ = input[\"motion\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c64a3-ce84-40d1-bbe3-541aa798a582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d934f4b8-0532-4a3f-9382-507fc14f2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = pos_emb(input[\"motion\"][0]).repeat(b , 1 ,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ce8b3c55-1c80-493f-bf3f-d1515d2e4807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 116, 256])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb8349-57bd-482e-b00a-36fcc5670155",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, (b , c) in conditions.items():\n",
    "    print(a)\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a53af4-0e65-4077-a1a2-a19f9cef26c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bb9f8cd0-8e0f-470a-8fc3-c02969b2f06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_ , cross_inputs = condition_fuser(x , conditions  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ee340a5a-6b33-43fc-8e74-e165409b7731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 153, 256])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2a8b199b-04a9-4597-90b0-ed420188c671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 500, 256])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62191059-8fe0-489d-9fd9-04bdc9dd805b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
