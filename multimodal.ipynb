{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231a3e8-9e24-4990-b940-00cdc6b15e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c44eef0-cea4-4ac9-b924-1327280f235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d81025f-b2fd-4067-9c7a-5c21f74f8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d36d72-1eba-4fd0-ba1e-69a4e714339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from functools import partial\n",
    "from torch import einsum, nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import pack, rearrange, reduce, repeat, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70de7dab-5b08-4278-872f-ca3885ac3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllFile(base):\n",
    "    file_path = []\n",
    "    for root, ds, fs in os.walk(base, followlinks=True):\n",
    "        for f in fs:\n",
    "            fullname = os.path.join(root, f)\n",
    "            file_path.append(fullname)\n",
    "    return file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395afd96-a4f7-45e3-b006-57abc3191800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.motion_processing.hml_process import recover_from_ric, recover_root_rot_pos,recover_from_rot\n",
    "import utils.vis_utils.plot_3d_global as plot_3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def vis(mot , dset , name = \"motion\"):\n",
    "\n",
    "    if isinstance(mot , torch.Tensor):\n",
    "        mot = dset.toMotion(mot)\n",
    "    mot =dset.inv_transform(mot)\n",
    "\n",
    "\n",
    "\n",
    "    xyz = np.array(dset.to_xyz(mot).cpu())\n",
    "\n",
    "    print(xyz.shape)\n",
    "\n",
    "    \n",
    "    plot_3d.render(xyz , f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/{name}.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f8324-7df7-471e-9832-56b3e3c4c9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96905ec0-51ae-4026-bf8c-624534a2002e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1774b40d-c90d-4ec3-a9fa-6c901473eb7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from configs.config_streaming import get_cfg_defaults as strm_get_cfg_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac75337e-08df-418c-854d-f0b05e9c9524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_cfg = strm_get_cfg_defaults()\n",
    "gen_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_streaming/motion_streaming.yaml\")\n",
    "gen_cfg.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d412fc-a32b-413d-8975-bd7300e1f256",
   "metadata": {},
   "source": [
    "## VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba58dbf4-e5be-4e12-a0a7-c0349927c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.resnetVQ.vqvae import HumanVQVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f3ff44-3e94-48bb-a0db-f2dd1a0cc536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "body_cfg = get_cfg_defaults()\n",
    "body_cfg.merge_from_file(gen_cfg.vqvae.body_config)\n",
    "left_cfg = get_cfg_defaults()\n",
    "left_cfg.merge_from_file(gen_cfg.vqvae.left_hand_config)\n",
    "right_cfg = get_cfg_defaults()\n",
    "right_cfg.merge_from_file(gen_cfg.vqvae.right_hand_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974ed84f-4fb4-4c59-ae89-7e9e6caae0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_hand_model = HumanVQVAE(left_cfg.vqvae).to(\"cpu\").eval()\n",
    "left_hand_model.load(os.path.join(left_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n",
    "right_hand_model = HumanVQVAE(right_cfg.vqvae).to(\"cpu\").eval()\n",
    "right_hand_model.load(os.path.join(right_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n",
    "body_model = HumanVQVAE(body_cfg.vqvae).to(\"cpu\").eval()\n",
    "body_model.load(os.path.join(body_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a89bfa-687b-44dd-b498-a43e0ef7b816",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Motion Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a735ceed-be47-460e-96c0-da344db04f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core import MotionRep, AudioRep, TextRep\n",
    "from core.datasets.conditioner import ConditionProvider, ConditionFuser\n",
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "from core.models.generation.motion_generator import Transformer, MotionMuse\n",
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dad8049-a4a0-4612-97ae-06e6e22ffb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config_t2m import cfg, get_cfg_defaults\n",
    "from configs.config import get_cfg_defaults as get_cfg_defaults3\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_generation/motion_generation.yaml\")\n",
    "cfg.freeze()\n",
    "mmuse_args = cfg.motion_generator\n",
    "dataset_args = cfg.dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c174676-b3a5-424c-a6a7-212b1960263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "target = mmuse_args.pop(\"target\")\n",
    "motion_muse = MotionMuse(mmuse_args).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8441fb76-675e-4500-ab2f-2d528507c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.resnetVQ.vqvae import HumanVQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e17fa70-7497-441b-bfc2-6f80c4315101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vcfg = get_cfg_defaults3()\n",
    "vcfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body_gprvc/vqvae_body_gprvc.yaml\")\n",
    "vqvae_args = vcfg.vqvae\n",
    "vqvae_args.nb_joints = 22\n",
    "vqvae_args.motion_dim = 263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2fcee-d2d4-4428-b66e-da75989882f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153aab1a-472c-4f32-9477-7cef6c60c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.split(np.cusum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5fb8d3b-a01c-4d29-8c2b-635e1daf80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = HumanVQVAE(vqvae_args).to(device).eval()\n",
    "vqvae_model.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ACMG/checkpoints/smplx_resnet/vqvae_motion.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ab9d2-a089-48ef-b155-7b58f558ddae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a1f7b4-6181-4bbf-be86-53f21f954486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            audio_padding=dataset_args.audio_padding,\n",
    "            motion_max_length_s=10,\n",
    "            audio_max_length_s=10,\n",
    "            pad_id = motion_muse.transformer.pad_token_id,\n",
    "            fps=30/4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b646cf7c-5536-43f8-98dc-335c8791ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bod_ind = np.load(\"/srv/hays-lab/scratch/sanisetty3/motionx/indices/body/aist/subset_0000/Dance_Break_3_Step_clip_1.npy\")\n",
    "# lh_ind = np.load(\"/srv/hays-lab/scratch/sanisetty3/motionx/indices/left_hand/aist/subset_0000/Dance_Break_3_Step_clip_1.npy\")\n",
    "# rh_ind = np.load(\"/srv/hays-lab/scratch/sanisetty3/motionx/indices/right_hand/aist/subset_0000/Dance_Break_3_Step_clip_1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e52a53-c80a-44cc-b2d3-c89a18eb081b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e30407-e8e1-4e05-8fe1-28f95c5eec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a0df4-f265-41b5-9577-5162b7e262f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4d76d-6cc5-4fe2-bd4b-7b5984f591eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e44106b1-f9a2-45cb-9a2e-e20773e110f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 265 and texts 265\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "\n",
    "dset = MotionIndicesAudioTextDataset(\"animation\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"full\", split = \"train\" , fps = 30/4  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f05f6af-6eec-4961-a203-8e3ec4d07d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpss  = next(iter(dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67996729-7f6f-4f75-a95f-adda0b498f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 73 and texts 73\n",
      "Total number of motions choreomaster: 34 and texts 34\n"
     ]
    }
   ],
   "source": [
    "train_ds, sampler_train, weights_train  = load_dataset_gen(dataset_args=dataset_args, split = \"train\" , dataset_names = [\"animation\" , \"choreomaster\" ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4dc88a-d090-4ebf-831e-b0054072e83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29b1d3c1-2084-4c57-bd85-b391f57e418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dset,\n",
    "        4,\n",
    "        # sampler=sampler_train,\n",
    "        # shuffle = False,\n",
    "        collate_fn=partial(simple_collate , conditioner = condition_provider),\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce18ef6b-8c40-4b3e-b4b4-704d756b863d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for inputs, conditions in train_loader:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c08cbf2-99c4-4355-8c0f-e1cb331e32b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 52, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"motion\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7693e02-e1e4-4840-b20f-2ab6a0cddd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 52])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"motion\"][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a017d85-0ab0-4542-8338-93e66c6efd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[\"audio\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22949664-9e06-41e8-9368-c533a5a445a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[\"text\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb4cb22e-4566-4f10-95a4-d3b6e62d3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "motions = inputs[\"motion\"][0].squeeze().to(torch.long)\n",
    "motion_mask = inputs[\"motion\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d6f90a96-5765-4831-a80a-6a4bb12e2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_method = {\"cross\": [\"audio\"], \"prepend\": [\"text\"]}\n",
    "condition_fuser = ConditionFuser(fuse_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41376d0a-fdff-4503-a5fb-38167a369969",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embed = self.project_audio(conditions[\"audio\"][0])\n",
    "text_embed = self.project_text(conditions[\"text\"][0])\n",
    "\n",
    "inputs_, cross_inputs = self.condition_fuser(\n",
    "    input,\n",
    "    {\n",
    "        \"text\": (text_embed, conditions[\"text\"][1]),\n",
    "        \"audio\": (audio_embed, conditions[\"audio\"][1]),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c731b-4240-4537-a4b7-0baff3c201e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ed176-a935-4fbd-93d6-b004c5768f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af848aa-93f8-487b-ad27-83c6ea2ec1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4261c42-9d36-49ee-b4de-b47996f33721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da28d2c-8543-4356-8644-37db1a506519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227c9af-4388-4331-b300-38aa8c292b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe478ffa-20be-4503-a905-c3d7e3105b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "832689e8-645f-4e2a-8fda-b696a57edee2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MotionMuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e21289cf-8d3d-4bd7-8294-8f526ecd4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss , logits = motion_muse((motions , motion_mask) , conditions , cond_drop_prob = 0.4 , return_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d7565e36-238b-4dab-8fc2-f0d626b1c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_indices = lologits.argmax(-1)\n",
    "pred_motion  = vqvae_model.decode(pred_indices[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e5cf9-2d15-4c2b-bd87-a8a09932ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "17c774f5-60ca-4131-ac61-4d8d288e9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_motion = torch.where(motions >= 1024 , 0 , motions)\n",
    "gt_motion  = vqvae_model.decode(mod_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a2f0d8a-23f2-4001-b561-a499280742cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 263])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ba8f456-13d1-4ee7-861e-35e7409b9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.render_hml(\n",
    "                    gt_motion[1][:(int(sum(motion_mask[1])) *4)].detach().squeeze().cpu(),\n",
    "                    \"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gt_motion_recon.gif\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908211ad-edac-4671-804c-82378aca2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = motion_muse.transformer.forward_with_cond_scale((motions , motion_mask) , conditions)\n",
    "logits3 =motion_muse.transformer.forward_with_neg_prompt((motions , motion_mask) , conditions , conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b4bafcb-4dae-41bd-bb35-4eec9411eff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 28])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12cc9378-24d1-4a80-8127-0f37701e8195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d4446cd2-9d83-4c0a-91fb-308d3a1fe43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "abdc0dd2-6fd3-4ee1-9c31-64d26aa47fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8823b0-c190-40b4-b8db-9af144202f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "941f570a-d0d2-4fca-8631-abb6f1e97084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 18/18 [00:01<00:00, 14.65it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_ids = motion_muse.generate(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad7f38-68a5-470f-be70-49271deb6e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88a8e7ed-a9d6-4fb5-a45a-187c48ce1f72",
   "metadata": {},
   "source": [
    "## Streaming transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1e61c8-9de6-4062-a444-aa0c41b2700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from core.param_dataclasses import pattern_providers\n",
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c76b02d-2c79-45aa-a014-dd65ec93cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import MotionRep, AudioRep, TextRep\n",
    "from core.datasets.conditioner import ConditionProvider,ConditionFuserStreamer\n",
    "from core.models.generation.lm import LMModel, MotionGen\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d9572-d125-45c7-9892-9383fefc386f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dec2ab9-311e-46a7-ae4c-262399be4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = strm_get_cfg_defaults()\n",
    "gen_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_streaming/motion_streaming.yaml\")\n",
    "gen_cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e7b5a3-d60d-4902-8e47-83797a5ed0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_args = gen_cfg.transformer_lm\n",
    "target = lm_args.pop(\"target\")\n",
    "fuse_config = gen_cfg.fuser\n",
    "pattern_args = gen_cfg.codebooks_pattern\n",
    "dataset_args = gen_cfg.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00544a2c-cf08-4e84-a84a-8313866c9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gen= MotionGen(lm_args , fuse_config , pattern_args ).to(device)\n",
    "model_gen = model_gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1c75f-9eec-478b-8655-a031016e326a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3749c8e-0eca-48fd-ba1b-3046cb83b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling = pattern_args.pop(\"modeling\")\n",
    "pattern_provider = pattern_providers[modeling](lm_args.n_q, delays = pattern_args.delays , flatten_first = pattern_args.flatten_first , empty_initial = pattern_args.empty_initial )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aabc42ef-4be4-4bd5-8618-6e6681e52706",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_method = fuse_config.pop(\"fuse_method\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6604e8ba-0e24-4722-a13e-78409cd89b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_method = {'cross': ['text'], 'input_interpolate': ['audio']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "33bb9d56-c713-404a-85e5-69c9b4b015a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(fuse_method, list):\n",
    "    fuse_method = fuse_method[0]\n",
    "condition_fuser = ConditionFuserStreamer(fuse_method, **fuse_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a3f9f-3461-4c9d-bf4d-236287f1a6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f861be10-8c39-4080-a103-855c3483316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LMModel(\n",
    "            pattern_provider=pattern_provider,\n",
    "            fuser=condition_fuser,\n",
    "            **lm_args\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "417a0493-4f58-478a-a699-02f62d0be73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a49bd9-1160-4181-8512-b6e4b3973a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3493b2d-800b-4c06-a117-c5a81b89ab73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b40dde6-abbd-41d5-82ec-2835fc4118e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            audio_padding=dataset_args.audio_padding,\n",
    "            motion_max_length_s=10,\n",
    "            audio_max_length_s=10,\n",
    "            pad_id = model_gen.model.pad_token_id,\n",
    "            fps=30/4,\n",
    "            # device = \"cpu\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc8d1fa-a114-4d34-9deb-77f40e573f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "# dset = MotionIndicesAudioTextDataset(\"chroeomaster\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"full\", split = \"train\" , fps = 30/4  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f678ef65-eb0b-4491-b383-3bc9af5a03ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 8 and texts 8\n",
      "Total number of motions choreomaster: 2 and texts 2\n"
     ]
    }
   ],
   "source": [
    "train_ds, sampler_train, weights_train  = load_dataset_gen(dataset_args=dataset_args, split = \"test\" , dataset_names = [\"animation\" , \"choreomaster\"] )\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        4,\n",
    "        # sampler=sampler_train,\n",
    "        # shuffle = False,\n",
    "        collate_fn=partial(simple_collate , conditioner = condition_provider , permute = True),\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cd33baa-e890-42a0-9f5e-2f470b5c9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fles = findAllFile(\"/srv/hays-lab/scratch/sanisetty3/motionx/indices/body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ab4620f-c0a6-4d5e-a075-1a097bc2a6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(fles[2000]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52109a38-0e3b-49a5-926b-3cbf7848f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████▋                                                       | 24685/80067 [00:17<00:35, 1582.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/scratch/sanisetty3/motionx/indices/body/animation/subset_0003/Ways_To_Stand_Groomsman.npy\n",
      "/srv/hays-lab/scratch/sanisetty3/motionx/indices/body/animation/subset_0003/Ways_To_Pick_Up_A_Dollar_Cartwheel.npy\n",
      "/srv/hays-lab/scratch/sanisetty3/motionx/indices/body/animation/subset_0003/Ways_To_Pick_Up_A_Dollar_Dramatic.npy\n",
      "/srv/hays-lab/scratch/sanisetty3/motionx/indices/body/animation/subset_0000/Ways_To_Enter_A_Room_Candy_Store.npy\n",
      "/srv/hays-lab/scratch/sanisetty3/motionx/indices/body/animation/subset_0002/Ways_To_Jump_In_Swim_Get_Out_Of_A_Pool_Too_Cold.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 80067/80067 [00:53<00:00, 1493.14it/s]\n"
     ]
    }
   ],
   "source": [
    "lenss = []\n",
    "for p in tqdm(fles):\n",
    "    try:\n",
    "        lenss.append(np.load(p).shape[-1])\n",
    "    except:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41369c4f-4407-47a6-a9e2-bc0820798cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe674256-814a-44a7-8d56-bf6b243d2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, conditions in train_loader:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05bc814c-aa4f-464e-9d76-471a18a2249e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['animation/subset_0000/Ways_To_Catch_Juggling',\n",
       "       'animation/subset_0003/Ways_To_Text_One_Minute',\n",
       "       'animation/subset_0001/Ways_To_Jump_Sit_Fall_Cool_Teacher',\n",
       "       'animation/subset_0003/Ways_To_Sit_Robot'], dtype='<U56')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b78ff3-d6c7-423c-b991-8fdeb637d096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d3ae41f-e73e-4543-814f-a2ce73f0b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask = inputs[\"motion\"][1]\n",
    "motions_or_ids = inputs[\"motion\"][0].cuda()\n",
    "B, K, T = motions_or_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda20bc2-4cce-4928-adbb-e537934504e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "motions = inputs[\"motion\"][0].squeeze().to(torch.long)\n",
    "motion_mask = inputs[\"motion\"][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e72a972-691e-4adb-9975-d9058cef37e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a619e0-8c14-4e64-acaa-de047244d59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f33b546b-0225-4bf1-a579-af34eba142c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 24, 768])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[\"text\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "212a21a7-3d15-4cc8-8a51-e283c4edc36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 128])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[\"audio\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d5f4c42-e0fb-4c14-88e6-c3b5998417c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[\"audio\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ea48a-12a1-4728-adfc-a8b9eaaa26f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2f0d8a60-d111-4f5b-8891-43893df1a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = conditions[\"audio\"][0]\n",
    "cond_mask = conditions[\"audio\"][1]\n",
    "cond = einops.rearrange(cond, \"b t d -> b d t\")\n",
    "cond = F.interpolate(cond, size=52)\n",
    "cond_mask = (\n",
    "    F.interpolate(\n",
    "        cond_mask.unsqueeze(1).to(torch.float),\n",
    "        size=52,\n",
    "    )\n",
    "    .squeeze(1)\n",
    "    .to(torch.bool)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cfdddd-0d8b-407e-a539-587692c88ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ae6d7-deb4-4e33-92ac-722d1dfd24ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c59afbaa-ec89-469c-9bc6-3ede8e91c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model_gen((motions, motion_mask), conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3354d-d97a-4921-a1bc-76a38edfec80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a4e90-37b9-453b-b7d9-7d1ca9fd9e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71844ff0-ff1d-48d4-bac9-693e550e9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "B , N , C = conditions[\"audio\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11627b3c-0670-42d6-8cf7-339080aff996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5963313-f768-4695-b785-db59931e2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.compute_predictions(inputs[\"motion\"] , conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716358b3-c74e-4c13-bf76-7856612b98cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3e160-dc04-412e-a0bc-2365e976cab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12f58033-d6e5-435f-9839-e0f2268c8292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e5f84-95e6-45c6-b610-abd08cc6e9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca3cc8-6c4a-4d17-910a-816df4dcdf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f752773-ff68-4d72-84a2-a27a927d5bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d71bc5ef-1191-49b3-b4fc-2922d6e0578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embed = model.project_audio(conditions[\"audio\"][0])\n",
    "text_embed = model.project_text(conditions[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b770ec8b-fb3e-4501-b11f-2e6fca2db741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 500, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13ffb358-c31e-48ac-a229-6bf2d976a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = sum([model.emb[k](motions_or_ids[:, k]) for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5981829a-08da-4d6a-ac68-4c9d6ddb40cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 52])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e04a5d55-dfb6-4635-a512-09636ab3c613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83474047-5877-4996-aef8-2159b756ccb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba86e5f-a77f-46df-80dd-49366cec791b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce92be-036b-4aa0-9024-4b90c4c8ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b74f0d70-c2f3-4f7b-bbee-7e97aa6e04d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38005fc1-078d-462e-8944-491ec9bae5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c7f886f9-98ab-4491-929f-134db0aad9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00f9a234-a968-4fa1-bee2-cee51940b89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d7c71-1288-41de-b3d6-aec9dadd3454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset = train_ds.datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c34dc-ddc3-4025-ae66-20e924d1c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_inds = motions_or_ids[:,0]\n",
    "left_inds = motions_or_ids[:,1]\n",
    "right_inds = motions_or_ids[:,2]\n",
    "body_motion = body_model.decode(body_inds[0:1]).detach()\n",
    "left_motion = left_hand_model.decode(left_inds[0:1]).detach()\n",
    "right_motion = right_hand_model.decode(right_inds[0:1]).detach()\n",
    "body_M = dset.toMotion(body_motion[0] , motion_rep = MotionRep(body_cfg.dataset.motion_rep) , hml_rep = body_cfg.dataset.hml_rep , )\n",
    "left_M = dset.toMotion(left_motion[0] , motion_rep = MotionRep(left_cfg.dataset.motion_rep) , hml_rep = left_cfg.dataset.hml_rep , )\n",
    "right_M = dset.toMotion(right_motion[0] , motion_rep = MotionRep(right_cfg.dataset.motion_rep) , hml_rep = right_cfg.dataset.hml_rep , )\n",
    "full_M = dset.to_full_joint_representation(body_M , left_M , right_M )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a603b399-f3c0-44d5-a131-7016920a7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.render_hml(full_M , \"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/full_joined.gif\", from_rotation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1081399a-65ae-4747-a56d-3802a0e6c144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b06d3ea-6ea0-4aa0-a555-3908fad8c480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6973207f-d18e-4d3c-ab30-f42f726d3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "choreo = np.load(\"/srv/hays-lab/scratch/sanisetty3/motionx/motion_data/new_joint_vecs/choreomaster/1160.npy\")\n",
    "full_og = dset.toMotion(choreo , motion_rep = MotionRep(\"full\") , hml_rep = \"gprvc\" )\n",
    "xyz = dset.to_xyz(full_og, from_rotation=True).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fcaf8a32-a788-4d08-bb6d-1bb311b4f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.vis_utils.plot_3d_global as plot_3d\n",
    "plot_3d.render(\n",
    "            np.array(xyz)[:300],\n",
    "            \"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/full_og.gif\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1ebaf935-e899-4010-9dfb-7c28416c4f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6466, 623)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choreo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4538193f-c369-4e1b-9d2a-5b4769cdf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_M = dset.inv_transform(full_M)\n",
    "full_M.tensor()\n",
    "full_M.root_params = full_og.root_params[:full_M.rotations.shape[0]]\n",
    "xyz_ = dset.to_xyz(full_M, from_rotation=True).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7585ca9-6488-49a7-8335-8eed85e2ec06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6448, 52, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ceec94b8-fadc-449c-97b8-4d8a617d3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_3d.render(\n",
    "            np.array(xyz_)[:300],\n",
    "            \"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/full_joined.gif\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabe69b-94cd-40fc-bd5f-e53f789116f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27527f-245e-4aa1-b13d-6b2b2bf06849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8976409d-c462-4ee2-b659-7f6628691592",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_conditions = model.cfg_dropout(conditions, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "420f1487-6119-4c26-927a-aa2b00f2885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out = model.compute_predictions(inputs[\"motion\"] , conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e0e834b-0d15-409b-a3b9-cea2fb0d50a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f4568d9-bc05-497c-a1f6-acd5daac1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outt = model.generate(conditions = null_conditions , two_step_cfg =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bae0039-4a36-4130-ba7a-7974783611bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 225])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c369b-96c3-4378-8464-50e639fb022f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1663e-84b2-4bac-83f2-c77eda12320e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9d584-1d77-4bac-9e50-6dbc8498e50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8470b40-073a-4353-a651-99af8be59f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom attention took 31.624648332595825 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    outt = model.generate(conditions = conditions , two_step_cfg =True)\n",
    "end = time.time()\n",
    "print('custom attention took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a3ff337-f1c2-4297-97e2-9a34bee74329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom attention without flash took 34.69120717048645 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    outt = model.generate(conditions = conditions , two_step_cfg =True)\n",
    "end = time.time()\n",
    "print('custom attention without flash took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b0f52-c008-408c-a191-222399b388f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e66e0dc-d743-4da6-afbd-3aacd5d918d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch attention took 22.785192251205444 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    outt = model.generate(conditions = conditions , two_step_cfg =True)\n",
    "end = time.time()\n",
    "print('torch attention took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00118e07-384d-45b0-b8fc-ecfda0545d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca6fa3-696e-44fa-a980-9d14e1564e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecdff3-035e-4de1-a9c7-e071841ffc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3f30e-3c3f-4c65-aa53-ffab3f9b032c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09223a9f-75d1-47d0-a5a8-e95f22bafbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f10cf3-7354-41a4-9387-2cbdc6e0311c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637bf6a-df0b-4cd8-a5df-93164ea093ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4feeba5d-cd88-4da8-b3b1-fa6aa37aed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = cfg_s.transformer_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e725d30c-b766-4f13-a77c-c9a6e1a8f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = MotionTokenizerParams(model_cfg.card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3de950ca-b389-492b-a5dd-442ffefc7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_q = 3\n",
    "codes = inputs[\"motion\"][0].permute(0,2,1).to(torch.long)\n",
    "code_mask = inputs[\"motion\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "91b54732-8ba6-4285-9c9f-9b2f438329f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, K, T = codes.shape\n",
    "codes = codes.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "315e1a5f-4d3b-469a-be18-acdc7d3653d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 52, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2bb884ad-ec84-4b05-8932-c879432c1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = pattern_provider.get_pattern(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bd4a31af-5302-4ac8-b55f-37ce3d633d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pattern(layout=[[], [LayoutCoord(t=0, q=0)], [LayoutCoord(t=1, q=0), LayoutCoord(t=0, q=1), LayoutCoord(t=0, q=2)], [LayoutCoord(t=2, q=0), LayoutCoord(t=1, q=1), LayoutCoord(t=1, q=2)], [LayoutCoord(t=3, q=0), LayoutCoord(t=2, q=1), LayoutCoord(t=2, q=2)]], timesteps=3, n_q=3)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2affa79-d5a2-4c1a-a109-c0c1a3fc8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_codes, sequence_indexes, sequence_mask = (\n",
    "            pattern.build_pattern_sequence(\n",
    "                codes,\n",
    "                1025,\n",
    "                keep_only_valid_steps=True,\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb69c415-f1c7-4344-bb23-8a71add2b544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 53])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66154dbc-7ca8-427a-9840-c09f2ea8f3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 52])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "273247a6-8017-423d-8a25-1acd03285ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 53])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e134f8e-af23-46e1-80f9-9505d5707e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask = torch.ones_like(sequence_mask).repeat(B , 1 , 1)\n",
    "for i in range(n_q):\n",
    "    new_mask[:,i,i+1:] = code_mask[:,0:T-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099dd667-48bf-455c-9ead-a85f92113b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "784fe530-6f4d-4dd2-8661-c97222c0da70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 53])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67188e6-85eb-41e4-b513-2cfc605bed54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a02a6b4-f643-40ec-931c-27be67587d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_mask = (new_mask.sum(1) == new_mask.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9728a38-64b0-4829-a2a9-de1a394e3b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_new_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba620f8-607f-4868-87d1-bdbd8bad7a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140792f0-0b9c-435e-88da-375174357c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.audio_encoders import EncodecConditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd45bec8-3c58-4545-9d18-2619f7a4bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list = findAllFile(\"/srv/hays-lab/scratch/sanisetty3/motionx/audio/wav/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48afcbc2-375e-4d95-b97a-6ef27a397749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "audenc = EncodecConditioner(target_sr = 9600)\n",
    "# audlib = AudioConditionerLibrosa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea049b53-c6df-4606-b859-6f34a96b1dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1724/1724 [07:27<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for src in tqdm(audio_list):\n",
    "    embs = audenc(audio_list[0])\n",
    "    np.save(src.replace(\"/wav\" , \"/encodec\").replace(\".wav\" , \".npy\") , embs.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f37c7075-36f4-4f16-a3b5-8f4bcfa86b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/hays-lab/scratch/sanisetty3/motionx/audio/wav/aist/mJS5.wav'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1746469b-975c-4f4e-a86d-44ea4f646cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = audenc(audio_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4793e041-2272-42cd-92a1-7b708300f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([887, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "401c23e5-bbbf-4351-adeb-bc8cb1afe27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2700, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"/srv/hays-lab/scratch/sanisetty3/motionx/audio/encodec/aist/mBR0.npy\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c17c903-36ce-4813-9cb9-152a4981422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = audenc(\"/srv/hays-lab/scratch/sanisetty3/motionx/audio/wav/choreomaster/0071.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19a7f591-a7e0-4722-8dcd-5d1c309e5495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb2 = audlib(\"/srv/hays-lab/scratch/sanisetty3/motionx/audio/wav/choreomaster/0071.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3a77f68-dd04-467d-8c5c-dc32e5dd48cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5234, 35)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef4f8574-6d24-4b59-a0a6-89a98b931161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8722, 128])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e882bae-0397-4dfc-928e-c8b6573bed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = np.load(\"/srv/hays-lab/scratch/sanisetty3/motionx/audio/encodec/choreomaster/0071.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a75f834f-ad5b-4cf5-96b0-255d87859e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8722, 128)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
