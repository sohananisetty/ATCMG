{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231a3e8-9e24-4990-b940-00cdc6b15e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c44eef0-cea4-4ac9-b924-1327280f235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d81025f-b2fd-4067-9c7a-5c21f74f8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d36d72-1eba-4fd0-ba1e-69a4e714339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from functools import partial\n",
    "from torch import einsum, nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70de7dab-5b08-4278-872f-ca3885ac3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllFile(base):\n",
    "    file_path = []\n",
    "    for root, ds, fs in os.walk(base, followlinks=True):\n",
    "        for f in fs:\n",
    "            fullname = os.path.join(root, f)\n",
    "            file_path.append(fullname)\n",
    "    return file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395afd96-a4f7-45e3-b006-57abc3191800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.motion_processing.hml_process import recover_from_ric, recover_root_rot_pos,recover_from_rot\n",
    "import utils.vis_utils.plot_3d_global as plot_3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def vis(mot , dset , name = \"motion\"):\n",
    "\n",
    "    if isinstance(mot , torch.Tensor):\n",
    "        mot = dset.toMotion(mot)\n",
    "    mot =dset.inv_transform(mot)\n",
    "\n",
    "\n",
    "\n",
    "    xyz = np.array(dset.to_xyz(mot).cpu())\n",
    "\n",
    "    print(xyz.shape)\n",
    "\n",
    "    \n",
    "    plot_3d.render(xyz , f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/{name}.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f8324-7df7-471e-9832-56b3e3c4c9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774b40d-c90d-4ec3-a9fa-6c901473eb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75337e-08df-418c-854d-f0b05e9c9524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9455c51-7edf-4641-a174-aea567bd6603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1523299e-4f73-48b0-8e16-03e03464321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "\n",
    "vcfg = get_cfg_defaults()\n",
    "vcfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body/vqvae_body_rv.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538bd7db-2d03-44e0-a757-98be718d734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.conditioner import ConditionProvider, ConditionFuser\n",
    "from core.datasets.multimodal_dataset import MotionAudioTextDataset, load_dataset, simple_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbffeb7-ce5f-4ee5-8c9f-280090d0729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.attend import Attention\n",
    "from core import AttentionParams\n",
    "from core import AttentionParams, TranslationTransformerParams, PositionalEmbeddingParams, PositionalEmbeddingType, MotionRep, AudioRep, TextRep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49f3ff-34de-4ab9-91d2-d7fa22f60310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c01eae-afd4-40a0-93d5-e157c3f081c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d84de22-8d50-499f-b8c9-4b36fc54b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.conditioner import ConditionProvider, ConditionFuser\n",
    "# from core.datasets.multimodal_dataset import MotionAudioTextDataset, load_dataset, simple_collate\n",
    "dataset_args = vcfg.dataset\n",
    "\n",
    "\n",
    "condition_provider2 = ConditionProvider(\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a6c40a-46d6-4224-8264-728c16e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.multimodal_dataset import MotionAudioTextDataset\n",
    "from core.datasets.vq_dataset import VQSMPLXMotionDataset\n",
    "from core.datasets.vq_dataset import load_dataset, simple_collate as simple_collate2\n",
    "from core import Motion\n",
    "\n",
    "from utils.motion_processing.skeleton import Skeleton, t2m_kinematic_chain , body_joints_id, t2m_raw_body_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db03f747-24d7-4ef0-81ed-27b999193a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = MotionAudioTextDataset(\"moyo\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"body\" , hml_rep = \"gprvc\", split = \"test\"   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88487a56-d2bf-4be8-a0d3-caa773a012db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions choreomaster: 34\n"
     ]
    }
   ],
   "source": [
    "dset = VQSMPLXMotionDataset(\"choreomaster\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"body\" , hml_rep = \"rv\", split = \"train\" , window_size = 600  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37ff16e6-0b76-40f3-a621-22c48e3163ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds, _, _ = load_dataset(\n",
    "#             dataset_args=dataset_args,\n",
    "#             split=\"test\",\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a913b-15de-48f4-8119-a4bd11148db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e823bc41-08e3-4b4d-ae60-923d5e00c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dset,\n",
    "        10,\n",
    "        # sampler=sampler,\n",
    "        collate_fn=partial(simple_collate2 , conditioner = condition_provider2),\n",
    "        # drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5510e5-d665-4f72-880c-366803188456",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs in train_loader:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c27946fe-185d-4751-9c5e-78b571d0579d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 300, 192])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot = inputs[\"motion\"][0]\n",
    "mot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f71a6d8-ea08-4318-8cef-30bbf340e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.render_hml(mot[0] , \"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/r.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638cad6-4505-4f8d-ba5a-294fbf856d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9b6be4e-9a49-4954-8feb-13f372ede150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.resnetVQ.vqvae import HumanVQVAE\n",
    "from core.models.loss import ReConsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ddacf-ce33-4659-a81a-56d068438e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74993259-b07b-4f4d-a975-025467755ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vqvae_args = vcfg.vqvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cb567ed-0fdb-4ab4-a87f-4148770c737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_args.nb_joints = dset.nb_joints\n",
    "vqvae_args.motion_dim = dset.motion_dim\n",
    "hml_rep = dset.hml_rep\n",
    "motion_rep = dset.motion_rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534ef5c-c9dc-43ea-8f63-16b4362287cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c385245a-2583-432d-9766-35a3c6244f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fnc = ReConsLoss(\"l1_smooth\" , True , vqvae_args.nb_joints , hml_rep=hml_rep , motion_rep = motion_rep  )\n",
    "loss_fnc2 = ReConsLoss(\"l1_smooth\" , False , vqvae_args.nb_joints , hml_rep=hml_rep , motion_rep = motion_rep  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "551ebd2e-ff11-41a1-980b-ae09617ac6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = HumanVQVAE(vqvae_args).to(device)\n",
    "vqvae_model.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body/vqvae_motion_rv.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0992432-a6d5-45a1-ab52-a473ef28fe78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87834859-4b88-4422-810f-c5ba1647c0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c05c20d6-13b2-486c-8c91-9642fce9b1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59a33e07-e8ef-4c26-8f81-72d93ccf68d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7518cf62-f15d-4f50-bc14-6db53ea053ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion = inputs[\"motion\"][0].to(device)\n",
    "out = vqvae_model(\n",
    "    motion=gt_motion,\n",
    "    # mask=mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35e540-5800-4c71-b188-6191cc8b5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion = inputs[\"motion\"][0].to(device)\n",
    "# pred = vqvae_model(motion)\n",
    "# loss = loss_fnc(pred.decoded_motion , motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3e99254-2b8b-4a47-a244-3ebf4af4a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_motion = loss_fnc(\n",
    "            out.decoded_motion, gt_motion, mask=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9939df00-8e8e-4e2a-b4de-28e8d3c57859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5a8e15e-88b2-4b17-97c5-563cfbf69069",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_motion2 = loss_fnc2(\n",
    "            out.decoded_motion, gt_motion, mask=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccd48501-432b-4d89-a95f-f2e0ae10f707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0545, device='cuda:0', grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_motion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb74a2-ba46-4264-9272-4b78c326b3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9b732-0cc0-4b68-b2a8-784e448c1786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee4d92a8-8311-4634-8c11-780f8f14b25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 49/49 [00:07<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vqvae_model.eval()\n",
    "val_loss_ae = {}\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(\n",
    "        (train_loader),\n",
    "        position=0,\n",
    "        leave=True,\n",
    "    ):\n",
    "        gt_motion = batch[\"motion\"][0].to(device)\n",
    "\n",
    "        indices = vqvae_model.encode(\n",
    "            motion=gt_motion,\n",
    "            # mask=mask,\n",
    "        )\n",
    "\n",
    "        used_indices = indices.flatten().tolist()\n",
    "        usage = len(set(used_indices)) / vqvae_args.codebook_size\n",
    "        # print(usage)\n",
    "\n",
    "        loss_dict = {\n",
    "            # \"total_loss\": loss.detach().cpu(),\n",
    "            # \"loss_motion\": loss_motion.detach().cpu(),\n",
    "            # \"commit_loss\": vqvae_output.commit_loss.detach().cpu(),\n",
    "            \"usage\": usage,\n",
    "        }\n",
    "\n",
    "        cnt+=1\n",
    "\n",
    "        for key, value in loss_dict.items():\n",
    "            if key in val_loss_ae:\n",
    "                val_loss_ae[key] += value\n",
    "            else:\n",
    "                val_loss_ae[key] = value\n",
    "\n",
    "\n",
    "for key in val_loss_ae.keys():\n",
    "    val_loss_ae[key] = val_loss_ae[key] / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ad3774-0bf4-43b6-aba2-a2b8ac428a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usage': 0.5708107461734694}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0b972-0c0a-4dd2-9172-81328aea0168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e09d6-84ef-49b1-897c-b4bba49a67d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b01a49-d07c-4f10-884a-f9a40e900425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12a89bfa-687b-44dd-b498-a43e0ef7b816",
   "metadata": {},
   "source": [
    "### Motion MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a735ceed-be47-460e-96c0-da344db04f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import MotionRep, AudioRep, TextRep\n",
    "from core.datasets.conditioner import ConditionProvider, ConditionFuser\n",
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "from core.models.generation.motion_generator import Transformer, MotionMuse\n",
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dad8049-a4a0-4612-97ae-06e6e22ffb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config_t2m import cfg, get_cfg_defaults\n",
    "from configs.config import get_cfg_defaults as get_cfg_defaults3\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_generation/motion_generation.yaml\")\n",
    "cfg.freeze()\n",
    "mmuse_args = cfg.motion_generator\n",
    "dataset_args = cfg.dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c174676-b3a5-424c-a6a7-212b1960263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = mmuse_args.pop(\"target\")\n",
    "motion_muse = MotionMuse(mmuse_args).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8441fb76-675e-4500-ab2f-2d528507c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.resnetVQ.vqvae import HumanVQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e17fa70-7497-441b-bfc2-6f80c4315101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vcfg = get_cfg_defaults3()\n",
    "vcfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body_gprvc/vqvae_body_gprvc.yaml\")\n",
    "vqvae_args = vcfg.vqvae\n",
    "vqvae_args.nb_joints = 22\n",
    "vqvae_args.motion_dim = 263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2fcee-d2d4-4428-b66e-da75989882f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153aab1a-472c-4f32-9477-7cef6c60c13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5fb8d3b-a01c-4d29-8c2b-635e1daf80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = HumanVQVAE(vqvae_args).to(device).eval()\n",
    "vqvae_model.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ACMG/checkpoints/smplx_resnet/vqvae_motion.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ab9d2-a089-48ef-b155-7b58f558ddae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a1f7b4-6181-4bbf-be86-53f21f954486",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            audio_padding=dataset_args.audio_padding,\n",
    "            motion_max_length_s=10,\n",
    "            audio_max_length_s=10,\n",
    "            pad_id = motion_muse.transformer.pad_token_id,\n",
    "            fps=30/4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646cf7c-5536-43f8-98dc-335c8791ac23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4d76d-6cc5-4fe2-bd4b-7b5984f591eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e44106b1-f9a2-45cb-9a2e-e20773e110f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions choreomaster: 34 and texts 34\n"
     ]
    }
   ],
   "source": [
    "dset = MotionIndicesAudioTextDataset(\"choreomaster\" , \"/srv/hays-lab/scratch/sanisetty3/motionx\" ,motion_rep = \"body\", split = \"train\" , fps = 30/4  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67996729-7f6f-4f75-a95f-adda0b498f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 308 and texts 308\n",
      "Total number of motions choreomaster: 34 and texts 34\n"
     ]
    }
   ],
   "source": [
    "train_ds, sampler_train, weights_train  = load_dataset_gen(dataset_args=dataset_args, split = \"train\" , dataset_names = [\"animation\" , \"choreomaster\" ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4dc88a-d090-4ebf-831e-b0054072e83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29b1d3c1-2084-4c57-bd85-b391f57e418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        4,\n",
    "        sampler=sampler_train,\n",
    "        # shuffle = False,\n",
    "        collate_fn=partial(simple_collate , conditioner = condition_provider),\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce18ef6b-8c40-4b3e-b4b4-704d756b863d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for inputs, conditions in train_loader:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c08cbf2-99c4-4355-8c0f-e1cb331e32b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 75, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"motion\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a017d85-0ab0-4542-8338-93e66c6efd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 500, 128])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[\"audio\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22949664-9e06-41e8-9368-c533a5a445a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb4cb22e-4566-4f10-95a4-d3b6e62d3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "motions = inputs[\"motion\"][0].squeeze().to(torch.long)\n",
    "motion_mask = inputs[\"motion\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f90a96-5765-4831-a80a-6a4bb12e2179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41376d0a-fdff-4503-a5fb-38167a369969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe478ffa-20be-4503-a905-c3d7e3105b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 75, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"motion\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1a5b350-263a-4ef6-9c7c-bf302a0d30b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 75])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e21289cf-8d3d-4bd7-8294-8f526ecd4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss , logits = motion_muse((motions , motion_mask) , conditions , cond_drop_prob = 0.4 , return_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d6e91-902c-4728-8289-7e7d518b53f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0fa35-3c18-43cc-95e7-9a3f018eb9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d7565e36-238b-4dab-8fc2-f0d626b1c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_indices = lologits.argmax(-1)\n",
    "pred_motion  = vqvae_model.decode(pred_indices[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e5cf9-2d15-4c2b-bd87-a8a09932ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "17c774f5-60ca-4131-ac61-4d8d288e9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_motion = torch.where(motions >= 1024 , 0 , motions)\n",
    "gt_motion  = vqvae_model.decode(mod_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a2f0d8a-23f2-4001-b561-a499280742cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 263])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ba8f456-13d1-4ee7-861e-35e7409b9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.render_hml(\n",
    "                    gt_motion[1][:(int(sum(motion_mask[1])) *4)].detach().squeeze().cpu(),\n",
    "                    \"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gt_motion_recon.gif\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f15d9-266b-4086-ac54-9c606ecab664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c33dd62-19dd-4cac-912e-d04a28d0cab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['animation/subset_0000/Ways_To_Catch_Juggling',\n",
       "       'animation/subset_0003/Ways_To_Text_One_Minute',\n",
       "       'animation/subset_0000/Ways_To_Go_To_Bed_Sleep_Wake_Up_Not_Enough_Blanket',\n",
       "       'animation/subset_0002/Ways_To_Open_A_Christmas_Gift_Kevinbparry'],\n",
       "      dtype='<U72')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663b1cc-2412-46f2-80d8-52243e9eac96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aed94f7e-2cd0-4c97-a62f-dbeb1194fac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd422d-3958-4c77-bd0c-391e5e22517d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17e1cc-b327-4c1a-ad0e-1448dba12c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3bd36-f97e-4734-98e0-b5b9fec0e0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908211ad-edac-4671-804c-82378aca2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = motion_muse.transformer.forward_with_cond_scale((motions , motion_mask) , conditions)\n",
    "logits3 =motion_muse.transformer.forward_with_neg_prompt((motions , motion_mask) , conditions , conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b4bafcb-4dae-41bd-bb35-4eec9411eff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 28])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12cc9378-24d1-4a80-8127-0f37701e8195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d4446cd2-9d83-4c0a-91fb-308d3a1fe43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "abdc0dd2-6fd3-4ee1-9c31-64d26aa47fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8823b0-c190-40b4-b8db-9af144202f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "941f570a-d0d2-4fca-8631-abb6f1e97084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 18/18 [00:01<00:00, 14.65it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_ids = motion_muse.generate(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad7f38-68a5-470f-be70-49271deb6e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba7b102f-d5b4-488e-9896-fa208cd203cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions[\"text\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65e0a8-6f4d-467f-ae3f-5ebabf0bbbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64e7eaf4-8d3e-44d0-a0c9-6809baa88a06",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301ed47-259c-4850-a3b0-370b41eab8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"a man dancing\"\n",
    "audio = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45e2f84f-c52a-475c-8614-fccf5bd609da",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud , am = condition_provider._get_audio_features(audio_list = [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7505174-0e14-4765-a886-e6b2de615524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 128)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72854d41-ae2e-4b22-9967-479e5151b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc74f7-78e4-4401-880d-7cb73ba151b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d5ff5-310c-4604-bdbe-2baa537d08c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a08eab58-a9aa-4b5f-8e8a-8f10b395752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sap = AttentionParams(dim = 256 , causal=True)\n",
    "cap = AttentionParams(dim = 256 , causal=True , add_null_kv=True)\n",
    "transformer_params = TranslationTransformerParams(self_attention_params = sap , \n",
    "                                                  cross_attention_params = cap , \n",
    "                                                  depth = 1, \n",
    "                                                  positional_embedding_params=PositionalEmbeddingParams(dim = 256) , \n",
    "                                                  positional_embedding=PositionalEmbeddingType.SINE,\n",
    "                                                  fuse_method = {\"cross_seperate\" : [\"audio\" , \"text\"]}\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630d974-93d0-4280-95bd-af6608b7c25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36579128-c825-4f7e-9416-b852d8536cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = transformer_blocks(\n",
    "            x=x_,\n",
    "            mask=x_padding_mask,\n",
    "            context=context,\n",
    "            context_mask=context_padding_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "557f7668-0ab2-47de-8f5b-3389a5c0751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embed[:, -n:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2e0c1b4-5cd3-494e-b89e-7cb40efd70fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 116, 256])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e4741-b259-4a50-a2ae-4ebf7ac1a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenCLIPEmbedder(nn.Module):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
    "    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77):\n",
    "        super().__init__()\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "        self.transformer = CLIPTextModel.from_pretrained(version)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.freeze()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.transformer = self.transformer.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
    "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "        outputs = self.transformer(input_ids=tokens)\n",
    "\n",
    "        z = outputs.last_hidden_state\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de48aa-bc99-4d66-921d-da96baa01e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38264d9e-7b0e-450c-a8fd-45464f90a4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16676f3f-c4e0-402e-8192-79b24528c866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f85cfd87-d174-419a-81d7-895447c01313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb = ScaledSinusoidalEmbedding(PositionalEmbeddingParams(dim = 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "983afa5c-2dc2-4733-8081-3e7e644bd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b , n , _ = input[\"motion\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c64a3-ce84-40d1-bbe3-541aa798a582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d934f4b8-0532-4a3f-9382-507fc14f2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = pos_emb(input[\"motion\"][0]).repeat(b , 1 ,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ce8b3c55-1c80-493f-bf3f-d1515d2e4807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 116, 256])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb8349-57bd-482e-b00a-36fcc5670155",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, (b , c) in conditions.items():\n",
    "    print(a)\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a53af4-0e65-4077-a1a2-a19f9cef26c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bb9f8cd0-8e0f-470a-8fc3-c02969b2f06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_ , cross_inputs = condition_fuser(x , conditions  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ee340a5a-6b33-43fc-8e74-e165409b7731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 153, 256])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2a8b199b-04a9-4597-90b0-ed420188c671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 500, 256])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62191059-8fe0-489d-9fd9-04bdc9dd805b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
