{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8138b8-714c-4bd5-bc6b-97cc9afa7a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e9292e-e925-4197-9b6d-c67ec0534a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e3b77a-d030-46b2-b0cf-8908bb4890fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b4be21-bd73-456d-9c1e-a74aa6f2c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from functools import partial\n",
    "from torch import einsum, nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import pack, rearrange, reduce, repeat, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3eec76-3888-498f-9e2e-65946d093030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.text_encoders import T5Conditioner,MPNETConditioner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ceccd3-4224-4a0d-baf7-dfccffb3a4af",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8975ceab-7c8f-4a1f-a2fe-d68257c91570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "def load_vqvae(gen_cfg):\n",
    "\n",
    "    body_cfg = get_cfg_defaults()\n",
    "    body_cfg.merge_from_file(gen_cfg.vqvae.body_config)\n",
    "    body_model = (\n",
    "        instantiate_from_config(body_cfg.vqvae).to(device).eval()\n",
    "    )\n",
    "    body_model.load(os.path.join(body_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n",
    "    if gen_cfg.vqvae.left_hand_config is  None and gen_cfg.vqvae.right_hand_config is None:\n",
    "        return body_model, body_cfg\n",
    "    \n",
    "    if gen_cfg.vqvae.left_hand_config is not None:\n",
    "        left_cfg = get_cfg_defaults()\n",
    "        left_cfg.merge_from_file(gen_cfg.vqvae.left_hand_config)\n",
    "        left_hand_model = instantiate_from_config(left_cfg.vqvae).to(device).eval()\n",
    "        left_hand_model.load(\n",
    "            os.path.join(left_cfg.output_dir, \"vqvae_motion.pt\")\n",
    "        )\n",
    "    else:\n",
    "        left_hand_model = None\n",
    "        \n",
    "    if gen_cfg.vqvae.right_hand_config is not None:\n",
    "        right_cfg = get_cfg_defaults()\n",
    "        right_cfg.merge_from_file(gen_cfg.vqvae.right_hand_config)\n",
    "        right_hand_model = instantiate_from_config(right_cfg.vqvae).to(device).eval()\n",
    "        right_hand_model.load(\n",
    "            os.path.join(right_cfg.output_dir, \"vqvae_motion.pt\")\n",
    "        )\n",
    "    else:\n",
    "        right_hand_model = None\n",
    "\n",
    "    return body_model, left_hand_model , right_hand_model , body_cfg , left_cfg , right_cfg\n",
    "\n",
    "def bkn_to_motion( codes, dset , remove_translation = True):\n",
    "    # codes b k n\n",
    "\n",
    "    k = codes.shape[1]\n",
    "    mrep = dset.motion_rep\n",
    "\n",
    "    if k == 1:\n",
    "        if mrep == MotionRep(\"body\"):\n",
    "\n",
    "            body_inds = codes[:, 0]\n",
    "            body_motion = body_model.decode(body_inds[0:1]).detach().cpu()\n",
    "\n",
    "            if remove_translation:\n",
    "                z = torch.zeros(\n",
    "                    body_motion.shape[:-1] + (2,),\n",
    "                    dtype=body_motion.dtype,\n",
    "                    device=body_motion.device,\n",
    "                )\n",
    "                body_motion = torch.cat(\n",
    "                    [body_motion[..., 0:1], z, body_motion[..., 1:]], -1\n",
    "                )\n",
    "\n",
    "            body_M = dset.toMotion(\n",
    "                body_motion[0],\n",
    "                motion_rep=MotionRep(\"body\"),\n",
    "                hml_rep=body_cfg.dataset.hml_rep,\n",
    "            )\n",
    "\n",
    "            return body_M\n",
    "\n",
    "        elif mrep == MotionRep(\"left_hand\"):\n",
    "\n",
    "            left_inds = codes[:, 0]\n",
    "            left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "            left_M = dset.toMotion(\n",
    "                left_motion[0],\n",
    "                motion_rep=MotionRep(left_cfg.dataset.motion_rep),\n",
    "                hml_rep=left_cfg.dataset.hml_rep,\n",
    "            )\n",
    "            return left_M\n",
    "\n",
    "        elif mrep == MotionRep(\"right_hand\"):\n",
    "            right_inds = codes[:, 0]\n",
    "            right_motion = (\n",
    "                right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "            )\n",
    "            right_M = dset.toMotion(\n",
    "                right_motion[0],\n",
    "                motion_rep=MotionRep(right_cfg.dataset.motion_rep),\n",
    "                hml_rep=right_cfg.dataset.hml_rep,\n",
    "            )\n",
    "            return right_M\n",
    "\n",
    "    if k == 2:\n",
    "        left_inds = codes[:, 0]\n",
    "        right_inds = codes[:, 1]\n",
    "\n",
    "        left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "        right_motion = right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "\n",
    "        left_M = dset.toMotion(\n",
    "            left_motion[0],\n",
    "            motion_rep=MotionRep(left_cfg.dataset.motion_rep),\n",
    "            hml_rep=left_cfg.dataset.hml_rep,\n",
    "        )\n",
    "        right_M = dset.toMotion(\n",
    "            right_motion[0],\n",
    "            motion_rep=MotionRep(right_cfg.dataset.motion_rep),\n",
    "            hml_rep=right_cfg.dataset.hml_rep,\n",
    "        )\n",
    "        hand_M = left_M + right_M\n",
    "        hand_M.motion_rep = MotionRep.HAND\n",
    "        hand_M.hml_rep = \"\".join(\n",
    "            [i for i in left_M.hml_rep if i in right_M.hml_rep]\n",
    "        )\n",
    "        return hand_M\n",
    "\n",
    "    if k == 3:\n",
    "        left_inds = codes[:, 1]\n",
    "        right_inds = codes[:, 2]\n",
    "        body_inds = codes[:, 0]\n",
    "        body_motion = body_model.decode(body_inds[0:1]).detach().cpu()\n",
    "\n",
    "        \n",
    "        if remove_translation:\n",
    "            z = torch.zeros(\n",
    "                body_motion.shape[:-1] + (2,),\n",
    "                dtype=body_motion.dtype,\n",
    "                device=body_motion.device,\n",
    "            )\n",
    "            body_motion = torch.cat([body_motion[..., 0:1], z, body_motion[..., 1:]], -1)\n",
    "\n",
    "        left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "        right_motion = right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "\n",
    "        body_M = dset.toMotion(\n",
    "            body_motion[0],\n",
    "            motion_rep=MotionRep(\"body\"),\n",
    "            hml_rep = body_cfg.dataset.hml_rep)\n",
    "\n",
    "        left_M = dset.toMotion(\n",
    "            left_motion[0],\n",
    "            motion_rep=MotionRep(\"left_hand\"),\n",
    "            hml_rep=left_cfg.dataset.hml_rep)\n",
    "        right_M = dset.toMotion(\n",
    "            right_motion[0],\n",
    "            motion_rep=MotionRep(\"right_hand\"),\n",
    "            hml_rep=right_cfg.dataset.hml_rep)\n",
    "        full_M = dset.to_full_joint_representation(body_M, left_M, right_M)\n",
    "        return full_M\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e36755-6af2-494a-aeb6-284f142732e8",
   "metadata": {},
   "source": [
    "## TMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7967e07f-6d65-481c-a0ee-2937b7209891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n",
    "from core.datasets.tmr_dataset import TMRDataset, load_dataset, simple_collate\n",
    "from core import MotionRep, TextRep, AudioRep\n",
    "from core.datasets.conditioner import ConditionProvider,ConditionFuser\n",
    "from configs.config_tmr import get_cfg_defaults as get_cfg_defaults_tmr\n",
    "from core.models.TMR.tmr import TMR\n",
    "from core.models.TMR.tmr import get_score_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ad421-c9f5-46b8-9812-879ccac41067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6681f-8f41-4f4f-a942-c29baea7eabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4742e48-d708-4488-9394-94b7130673c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b957f-4315-45de-ba0e-2870ddc07124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2f864f-15d4-4dcf-8b8d-da939b66c1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmr_cfg = get_cfg_defaults_tmr()\n",
    "tmr_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/tmr/tmr.yaml\")\n",
    "tmr_cfg.freeze()\n",
    "dataset_args = tmr_cfg.dataset\n",
    "tmr_parms = tmr_cfg.tmr\n",
    "\n",
    "\n",
    "_ = tmr_parms.pop(\"target\")\n",
    "motion_encoder = instantiate_from_config(tmr_cfg.motion_encoder).to(device)\n",
    "text_encoder = instantiate_from_config(tmr_cfg.text_encoder).to(device)\n",
    "motion_decoder = instantiate_from_config(tmr_cfg.motion_decoder).to(device)\n",
    "tmr = TMR(motion_encoder , text_encoder , motion_decoder , lr = tmr_cfg.train.learning_rate, **tmr_parms).to(device).eval()\n",
    "pkg = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/tmr/tmr.pt\")\n",
    "tmr.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21716048-3b60-4717-b5d5-f0880baccebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8fb7234-2882-47ab-bb11-5619e19c1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents(inputs , conditions):\n",
    "    text_conds = conditions[\"text\"]\n",
    "    text_x_dict = {\"x\": text_conds[0], \"mask\": text_conds[1].to(torch.bool)}\n",
    "    motion_x_dict = {\"x\": inputs[0], \"mask\": inputs[1].to(torch.bool)}\n",
    "    motion_mask = motion_x_dict[\"mask\"]\n",
    "    text_mask = text_x_dict[\"mask\"]\n",
    "    t_motions, t_latents, t_dists = tmr(\n",
    "            text_x_dict, mask=motion_mask, return_all=True, sample_mean = True\n",
    "        )\n",
    "\n",
    "    # motion -> motion\n",
    "    m_motions, m_latents, m_dists = tmr(\n",
    "        motion_x_dict, mask=motion_mask, return_all=True, sample_mean = True\n",
    "    )\n",
    "    return t_latents , m_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed256d-a248-4716-a5e8-9c6524a130d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f2f7c-8cb3-491a-b138-61a6e2c2e871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525e9e9-23b9-47d3-b876-fb8f6f9ab9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_names = [\"humanml\"] ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d6821a6-6b7f-443c-964f-aace12c3acec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 2 and texts 2\n",
      "Total number of motions humanml: 8802 and texts 8802\n",
      "Total number of motions perform: 16 and texts 16\n",
      "Total number of motions GRAB: 67 and texts 67\n",
      "Total number of motions idea400: 577 and texts 577\n",
      "Total number of motions humman: 20 and texts 20\n",
      "Total number of motions beat: 162 and texts 162\n",
      "Total number of motions game_motion: 131 and texts 131\n",
      "Total number of motions music: 148 and texts 148\n",
      "Total number of motions aist: 61 and texts 61\n",
      "Total number of motions fitness: 572 and texts 572\n",
      "Total number of motions moyo: 9 and texts 9\n",
      "Total number of motions choreomaster: 2 and texts 2\n",
      "Total number of motions dance: 7 and texts 7\n",
      "Total number of motions kungfu: 42 and texts 42\n",
      "Total number of motions EgoBody: 49 and texts 49\n",
      "Total number of motions HAA500: 17 and texts 17\n"
     ]
    }
   ],
   "source": [
    "train_ds, sampler_train, weights_train  = load_dataset( dataset_args=dataset_args, split = \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c0846-4bdd-4837-b273-f70a12df8631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c0425f1-f3a5-4759-bf91-efc58902bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            text_conditioner_name = dataset_args.text_conditioner_name,\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            motion_max_length_s=dataset_args.motion_max_length_s,\n",
    "            fps=30,\n",
    "            # only_motion = True\n",
    "        )\n",
    "condition_provider.audio_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fa2b85e-c70e-4ba9-98f8-f71df0fd8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        400,\n",
    "        # sampler=sampler_train,\n",
    "        # shuffle = False,\n",
    "        collate_fn=partial(simple_collate , conditioner = condition_provider),\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2a29728-a7fe-43e8-ab2b-63471be5bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, conditions in (train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48def63b-97b2-4cb6-837d-d4296b1f6e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5a0c5-28cb-48ca-a99e-448313038f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "111553d7-8161-4d98-b73c-0eaa241ec81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = inputs[\"motion\"][0]\n",
    "motion_mask = inputs[\"motion\"][1].to(torch.bool)\n",
    "text = conditions[\"text\"][0]\n",
    "text_mask = conditions[\"text\"][1].to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8edd4834-101b-4a80-9e80-361b1b8770a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 252])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0266a5ae-a146-45cb-9d00-fdcafd773c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recons': tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'kl': tensor(74.1251, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'latent': tensor(1.0820, device='cuda:0', grad_fn=<SmoothL1LossBackward0>),\n",
       " 'contrastive': tensor(0., device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss': tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = tmr.compute_loss(inputs[\"motion\"] , conditions)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aae80dab-42b2-49ca-9745-2ab20ad99f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8913, 0.5509, 0.5515, 0.7459],\n",
       "        [0.5962, 0.8877, 0.4547, 0.6056],\n",
       "        [0.5355, 0.4400, 0.8875, 0.6353],\n",
       "        [0.5808, 0.6178, 0.5276, 0.9520]], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_latents , m_latents = get_latents(inputs[\"motion\"],conditions)\n",
    "score = get_score_matrix(t_latents, m_latents).cpu()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c1721d8-5d4b-422c-9324-4d126d3bfa57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6846c6-b958-45fe-8e75-08a717c15ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f216fca-1219-47ce-817f-5496e873e19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a806869-58a1-4ee3-9a13-1b5227bded8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4d8b0e6-ea5e-45f0-8ea3-5e04fa6452ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "630efeb2-be7c-4ad3-ac1e-5686e198cd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e16941-4ab9-4502-8c83-8b4fd929f757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ca228-3199-4c19-9007-fe08d2c3eba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79b775a9-8696-4a45-b850-cb13eb3a02be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34113e14-9675-4914-9443-b0bf5516c4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e46dbbc-b309-4f3a-94f0-79c70fa01102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bb6e1-be80-4ce4-b846-26793863e94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069dcc3e-dc06-4694-8a6c-759234874651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5249b-4c80-4762-b6fb-e661e6aa6a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d388de8-8d9b-46ae-85a0-80ce1c6b87a1",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e87b442-10d3-4d4b-b9d8-b6c330b7b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.TMR.tmr import get_score_matrix\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6368d74b-e83a-4201-ba79-aae08c3b82b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# condition_provider = ConditionProvider(\n",
    "#             text_conditioner_name = dataset_args.text_conditioner_name,\n",
    "#             motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "#             audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "#             text_rep=TextRep(dataset_args.text_rep),\n",
    "#             motion_padding=dataset_args.motion_padding,\n",
    "#             motion_max_length_s=dataset_args.motion_max_length_s,\n",
    "#             fps=30,\n",
    "#             # only_motion = True\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac0db1e5-536a-4290-a3c2-f974481ea8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.datasets.base_dataset import BaseMotionDataset\n",
    "# base_dset = BaseMotionDataset(motion_rep=MotionRep.FULL , hml_rep= \"gprvc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd602886-407e-4085-a0ad-83859bbc8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(text:str , motion_path:str, return_embeds = False):\n",
    "    motion = np.load(motion_path)\n",
    "\n",
    "    text_embed, text_mask = condition_provider._get_text_features(\n",
    "        raw_text=text,\n",
    "    )\n",
    "    processed_motion = base_dset.get_processed_motion(\n",
    "            motion, motion_rep=MotionRep(dataset_args.motion_rep), hml_rep=dataset_args.hml_rep\n",
    "        )\n",
    "    mot_body = torch.Tensor(processed_motion()).to(device)[None]\n",
    "    print(mot_body.shape)\n",
    "    motion_x_dict = {\"x\": mot_body, \"mask\": torch.ones_like(mot_body)[...,0].to(torch.bool)}\n",
    "    text_x_dict = {\"x\": text_embed, \"mask\": text_mask.to(torch.bool)}\n",
    "    with torch.no_grad():\n",
    "        lat_m = tmr.encode(motion_x_dict, sample_mean=True)[0]\n",
    "        lat_t = tmr.encode(text_x_dict, sample_mean=True)[0]\n",
    "        score = get_score_matrix(lat_t, lat_m).cpu()\n",
    "\n",
    "    if return_embeds:\n",
    "        return score ,lat_m ,lat_t\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3ba3c-e207-499d-b2ef-ce489761561f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a86c1844-6823-4d4a-9de2-f323f830aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"a man sets to do a backflips then fails back flip and falls to the ground\"\n",
    "mot=\"/srv/hays-lab/scratch/sanisetty3/motionx/motion_data/new_joint_vecs/humanml/001034.npy\"\n",
    "text2 = \"a person standing loses balance falling to the right and recovers standing\"\n",
    "mot2=\"/srv/hays-lab/scratch/sanisetty3/motionx/motion_data/new_joint_vecs/humanml/000012.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a06df4a-9831-4f7e-be47-07ac7c272b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = np.load(mot)\n",
    "motion2 = np.load(mot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "347af130-3f12-4742-8aea-64fa378b7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 120, 137])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7139)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getScore(text , motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f3e94ca4-7c8e-4fea-947a-974956aad729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 120, 137])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9692)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getScore(text2 , motion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e72932-f9b1-45e6-958c-13a8b7ff6d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00413697-fd2b-48d8-93a3-14ac214c0053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ced5dfa-75f1-48fd-9268-737a055da2da",
   "metadata": {},
   "source": [
    "## Muse Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d21be686-931f-433d-a64c-1cd38813f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.generation.muse2 import generate_animation\n",
    "from core import MotionTokenizerParams, pattern_providers\n",
    "\n",
    "from core.param_dataclasses import pattern_providers\n",
    "# from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n",
    "# from core import MotionRep, AudioRep, TextRep\n",
    "# from core.datasets.conditioner import ConditionProvider,ConditionFuser\n",
    "from core.models.generation.muse2 import MotionMuse as MotionMuse2\n",
    "import einops\n",
    "from configs.config_t2m import get_cfg_defaults as muse_get_cfg_defaults\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "546a5812-7558-4d4e-b466-ead30e4811c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_cfg = muse_get_cfg_defaults()\n",
    "gen_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_muse_body_hands/motion_muse_body_hands.yaml\")\n",
    "gen_cfg.freeze()\n",
    "tranformer_config = gen_cfg.motion_generator\n",
    "fuse_config = gen_cfg.fuser\n",
    "pattern_config = gen_cfg.codebooks_pattern\n",
    "# dataset_args = gen_cfg.dataset\n",
    "\n",
    "target = tranformer_config.pop(\"target\")\n",
    "motion_gen = MotionMuse2(tranformer_config , fuse_config , pattern_config).to(device).eval()\n",
    "pkg = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_muse_body_hands/motion_muse.pt\", map_location=\"cuda\")\n",
    "motion_gen.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39ff8875-8aef-4813-8754-8a792ed45ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n",
      "loaded model with  0.03015906736254692 tensor([110000.], device='cuda:0') steps\n"
     ]
    }
   ],
   "source": [
    "body_model, left_hand_model , right_hand_model , body_cfg , left_cfg , right_cfg = load_vqvae(gen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc341171-18e4-4a3a-b1e6-c0c7024e21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.base_dataset import BaseMotionDataset\n",
    "base_dset = BaseMotionDataset(motion_rep=MotionRep.BODY , hml_rep= \"gpvc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bfe6a-4f2e-4e2f-8c65-3048bdc59673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd8d21-8cae-44b5-84d6-b959ee16a691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8537f1d-e1a7-479d-9821-83a8b5696c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(183.)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"lens\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b381ca6e-6a0b-43bf-957d-9b7a98a95915",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = inputs[\"texts\"][-1]\n",
    "duration_s = inputs[\"lens\"][-1]//30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "67ebac02-2f96-4874-9a8c-e670640a8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_s = int(inputs[\"lens\"].min()//30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8ba537a9-3817-4b6a-a362-ff98a92eb891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 36.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 37.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 37.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 37.02it/s]\n"
     ]
    }
   ],
   "source": [
    "all_ids_body = generate_animation(motion_gen , condition_provider ,overlap = 15, duration_s = duration_s ,  text = text_ , use_token_critic = True, timesteps = 24 )\n",
    "gen_motion = bkn_to_motion(all_ids_body[:,:1], base_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd22349-ae5f-46c3-8310-93eac89c54f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2faf152d-1acb-4483-b070-86dd0e1d95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_motion = bkn_to_motion(all_ids_body[:,:1], base_dset)\n",
    "# base_dset.render_hml(\n",
    "#                     gen_motion,\n",
    "#                     f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gen_novel_full.gif\",\n",
    "#                     zero_trans = True,\n",
    "#                     zero_orient = True,\n",
    "    \n",
    "#                 )\n",
    "# Image(open(f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gen_novel_full.gif\",'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bad359-dae1-49d0-9fb8-6628875082b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d89f07-17cf-4547-b57b-2a365daccb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567786ac-70c0-4c50-8d9f-471b08ceb4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b19ffe-7a80-4af6-a0d5-3a56764e7dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b5fd1-3593-4dbf-a7e1-8993912ed5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f397291-f00a-4ac5-bc42-b0c64a048fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.eval.eval_text.eval_text import evaluation_transformer, evaluation_vqvae\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f092969-cf75-4df2-b207-e931e259e6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n",
      "loaded model with  0.031938087195158005 tensor([110000.], device='cuda:0') steps\n"
     ]
    }
   ],
   "source": [
    "body_cfg_1024 = get_cfg_defaults()\n",
    "body_cfg_1024.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body_gpvc_1024/vqvae_body_gpvc_1024.yaml\")\n",
    "body_model_1024 = (instantiate_from_config(body_cfg_1024.vqvae).to(device).eval())\n",
    "body_model_1024.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_body_gpvc_1024/checkpoints/vqvae_motion.110000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbffcabc-d057-4b94-97b9-11d0638d26e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 26/26 [01:44<00:00,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->  FID. 73.3502, Diversity Real. 76.9231, Diversity. 76.9680, R_precision_real. [0.17432692 0.32038462 0.45451923], R_precision. [0.14336538 0.27182692 0.39048077], matching_score_real. 36.697810340294474, matching_score_pred. 40.861751051682695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from core.eval.eval_text.eval_text import evaluation_vqvae\n",
    "real_metrics, pred_metrics = evaluation_vqvae(val_loader = train_loader , \n",
    "                                                    motion_vqvae = body_model_1024, \n",
    "                                                    tmr = tmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db01795b-d4d4-4de9-8d7f-ec9437604c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.eval.eval_text.eval_text import evaluation_transformer, evaluation_vqvae\n",
    "# real_metrics, pred_metrics = evaluation_transformer(val_loader = train_loader , \n",
    "#                                                     condition_provider = condition_provider , \n",
    "#                                                     bkn_to_motion = bkn_to_motion, \n",
    "#                                                     motion_generator = motion_gen, \n",
    "#                                                     tmr = tmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fabb7e-9995-489c-bf26-08141511d71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c0960-9b48-4be7-ad9e-0a44090bd568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db3694-f712-4e0f-9db8-5b732c598572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d786017-5fe8-4963-9b32-ca8e819c9f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5168dba-65f1-46ea-9d81-f23bd36275c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6de37-18cf-4dcc-988a-6dcda45eb907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45326992-7e38-42df-9fec-d2a6c87aea6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2bf33308-d062-4cd8-916c-8c1c91e8e544",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 27.62 MiB is free. Including non-PyTorch memory, this process has 10.72 GiB memory in use. Of the allocated memory 9.41 GiB is allocated by PyTorch, and 303.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t_latents, m_latents \u001b[38;5;241m=\u001b[39m \u001b[43mget_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmotion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mget_latents\u001b[0;34m(inputs, conditions)\u001b[0m\n\u001b[1;32m      7\u001b[0m t_motions, t_latents, t_dists \u001b[38;5;241m=\u001b[39m tmr(\n\u001b[1;32m      8\u001b[0m         text_x_dict, mask\u001b[38;5;241m=\u001b[39mmotion_mask, return_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# motion -> motion\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m m_motions, m_latents, m_dists \u001b[38;5;241m=\u001b[39m \u001b[43mtmr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmotion_x_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmotion_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t_latents , m_latents\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/coc/scratch/sanisetty3/music_motion/ATCMG/core/models/TMR/temos.py:160\u001b[0m, in \u001b[0;36mTEMOS.forward\u001b[0;34m(self, inputs, lengths, mask, sample_mean, fact, return_all)\u001b[0m\n\u001b[1;32m    156\u001b[0m latent_vectors, distributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m    157\u001b[0m     inputs, sample_mean\u001b[38;5;241m=\u001b[39msample_mean, fact\u001b[38;5;241m=\u001b[39mfact, return_distribution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Decoding the latent vector: generating motions\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m motions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_all:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m motions, latent_vectors, distributions\n",
      "File \u001b[0;32m/coc/scratch/sanisetty3/music_motion/ATCMG/core/models/TMR/temos.py:142\u001b[0m, in \u001b[0;36mTEMOS.decode\u001b[0;34m(self, latent_vectors, lengths, mask)\u001b[0m\n\u001b[1;32m    140\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m length_to_mask(lengths, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    141\u001b[0m z_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m: latent_vectors, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m: mask}\n\u001b[0;32m--> 142\u001b[0m motions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmotion_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m motions\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/coc/scratch/sanisetty3/music_motion/ATCMG/core/models/TMR/actor.py:147\u001b[0m, in \u001b[0;36mACTORStyleDecoder.forward\u001b[0;34m(self, z_dict)\u001b[0m\n\u001b[1;32m    143\u001b[0m time_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_pos_encoding(time_queries)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Pass through the transformer decoder\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# with the latent vector for memory\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqTransDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(output)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# zero for padded area\u001b[39;00m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/transformer.py:465\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    462\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 465\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/transformer.py:855\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    853\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 855\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    856\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    857\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/transformer.py:864\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    863\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 864\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/functional.py:5336\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5335\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5336\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5338\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/torch/nn/functional.py:4859\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4857\u001b[0m     proj \u001b[38;5;241m=\u001b[39m linear(q, w, b)\n\u001b[1;32m   4858\u001b[0m     \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[0;32m-> 4859\u001b[0m     proj \u001b[38;5;241m=\u001b[39m \u001b[43mproj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proj[\u001b[38;5;241m0\u001b[39m], proj[\u001b[38;5;241m1\u001b[39m], proj[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   4861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4862\u001b[0m     \u001b[38;5;66;03m# encoder-decoder attention\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 27.62 MiB is free. Including non-PyTorch memory, this process has 10.72 GiB memory in use. Of the allocated memory 9.41 GiB is allocated by PyTorch, and 303.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "t_latents, m_latents = get_latents(inputs[\"motion\"], conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b467fc1-8614-4c32-a39b-dfe6222b5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_latents_pred, m_latents_pred = get_latents(\n",
    "                (pred_motion, inputs[\"motion\"][1]), conditions\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb413da8-1ab8-4347-a81d-07413e8aa5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score_matrix(t_latents_pred, m_latents_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b4f29-c00f-4579-b54e-d15155ce543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score_matrix(t_latents, m_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "27d543b4-23b9-494f-9627-4b2e311450d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2110, device='cuda:0', grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.smooth_l1_loss(m_latents_pred , m_latents )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70371ca2-e332-4578-aba6-907b0e101869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519c3b7-1b43-41b6-9570-8ea6a6d0120e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f02018-28d7-4536-9bf5-4727c5853120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98c59d-618c-443a-9397-911423ccc4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e643346-bab8-46bd-b34d-0179fc374816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c59b3-fa60-409c-a7b3-34d5538a01b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49efcfcb-b03b-46d7-b6bc-4c9c90738398",
   "metadata": {},
   "source": [
    "### Eval script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "391667b8-1106-45a7-8b8b-c0bcd54a617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents(inputs , conditions):\n",
    "    text_conds = conditions[\"text\"]\n",
    "    text_x_dict = {\"x\": text_conds[0], \"mask\": text_conds[1].to(torch.bool)}\n",
    "    motion_x_dict = {\"x\": inputs[0], \"mask\": inputs[1].to(torch.bool)}\n",
    "    motion_mask = motion_x_dict[\"mask\"]\n",
    "    text_mask = text_x_dict[\"mask\"]\n",
    "    t_motions, t_latents, t_dists = tmr(\n",
    "            text_x_dict, mask=motion_mask, return_all=True\n",
    "        )\n",
    "\n",
    "    # motion -> motion\n",
    "    m_motions, m_latents, m_dists = tmr(\n",
    "        motion_x_dict, mask=motion_mask, return_all=True\n",
    "    )\n",
    "    return t_latents , m_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e60f5567-0757-4417-bec4-8399adee200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.eval.helpers import calculate_activation_statistics, calculate_diversity, calculate_R_precision, calculate_frechet_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd0f8c7f-950e-4a9a-9e4e-8a917b35b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 30.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 31.61it/s]\n"
     ]
    }
   ],
   "source": [
    "motion_list = []\n",
    "motion_pred_list = []\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "with torch.no_grad():\n",
    "    \n",
    "    t_latents , m_latents = get_latents(inputs[\"motion\"],conditions)\n",
    "    score = get_score_matrix(t_latents, m_latents).cpu()\n",
    "    bs = inputs[\"motion\"][0].shape[0]\n",
    "\n",
    "    pred_pose_eval = torch.zeros_like(inputs[\"motion\"][0])\n",
    "    for k in range(4):\n",
    "        lenn = int(inputs[\"lens\"][k])\n",
    "        text_ = inputs[\"texts\"][k]\n",
    "        duration_s = math.ceil(lenn/30)\n",
    "        all_ids_body = generate_animation(motion_gen , condition_provider ,temperature = 0.6, overlap = 10, duration_s = duration_s ,  text = text_ , use_token_critic = True, timesteps = 24 )\n",
    "        gen_motion = bkn_to_motion(all_ids_body[:,:1], base_dset)\n",
    "        pred_pose_eval[k:k+1,:lenn] = gen_motion()[:lenn][None]\n",
    "\n",
    "    t_latents_pred , m_latents_pred = get_latents((pred_pose_eval , inputs[\"motion\"][1] ), conditions)\n",
    "    score_pred = get_score_matrix(t_latents_pred, m_latents_pred).cpu()\n",
    "\n",
    "    motion_list.append(m_latents)\n",
    "    motion_pred_list.append(m_latents_pred)\n",
    "\n",
    "    temp_R, temp_match = calculate_R_precision(\n",
    "    t_latents.cpu().numpy(), m_latents.cpu().numpy(), top_k=3, sum_all=True\n",
    "        )\n",
    "    R_precision_real += temp_R\n",
    "    matching_score_real += temp_match\n",
    "    temp_R, temp_match = calculate_R_precision(\n",
    "        t_latents_pred.cpu().numpy(), m_latents_pred.cpu().numpy(), top_k=3, sum_all=True\n",
    "        )\n",
    "    R_precision += temp_R\n",
    "    matching_score_pred += temp_match\n",
    "    nb_sample += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d1da372-8e44-47f6-945d-f3dc7b3d0b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "920a3276-27ab-4e0b-bfb0-d61892c293c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_annotation_np = torch.cat(motion_list, dim=0).cpu().numpy()\n",
    "motion_pred_np = torch.cat(motion_pred_list, dim=0).cpu().numpy()\n",
    "gt_mu, gt_cov = calculate_activation_statistics(motion_annotation_np)\n",
    "mu, cov = calculate_activation_statistics(motion_pred_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ade33622-6280-411c-b142-92f72dd3def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sample = motion_annotation_np.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc359b41-5ec2-4db8-831f-9f366b6118cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 256)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_annotation_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fdf52298-e9af-45b8-84ce-c8ef84cc3ebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Imaginary component 3.766078257669414e+24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m matching_score_real \u001b[38;5;241m=\u001b[39m matching_score_real \u001b[38;5;241m/\u001b[39m nb_sample\n\u001b[1;32m     10\u001b[0m matching_score_pred \u001b[38;5;241m=\u001b[39m matching_score_pred \u001b[38;5;241m/\u001b[39m nb_sample\n\u001b[0;32m---> 12\u001b[0m fid \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_frechet_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_cov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/scratch/sanisetty3/music_motion/ATCMG/core/eval/helpers.py:106\u001b[0m, in \u001b[0;36mcalculate_frechet_distance\u001b[0;34m(mu1, sigma1, mu2, sigma2, eps)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(np\u001b[38;5;241m.\u001b[39mdiagonal(covmean)\u001b[38;5;241m.\u001b[39mimag, \u001b[38;5;241m0\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m):\n\u001b[1;32m    105\u001b[0m         m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(covmean\u001b[38;5;241m.\u001b[39mimag))\n\u001b[0;32m--> 106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImaginary component \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m))\n\u001b[1;32m    107\u001b[0m     covmean \u001b[38;5;241m=\u001b[39m covmean\u001b[38;5;241m.\u001b[39mreal\n\u001b[1;32m    109\u001b[0m tr_covmean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtrace(covmean)\n",
      "\u001b[0;31mValueError\u001b[0m: Imaginary component 3.766078257669414e+24"
     ]
    }
   ],
   "source": [
    "# diversity_real = calculate_diversity(\n",
    "#     motion_annotation_np, 300 if nb_sample > 300 else 100\n",
    "# )\n",
    "# diversity = calculate_diversity(motion_pred_np, 300 if nb_sample > 300 else 100)\n",
    "\n",
    "R_precision_real = R_precision_real / nb_sample\n",
    "R_precision = R_precision / nb_sample\n",
    "\n",
    "matching_score_real = matching_score_real / nb_sample\n",
    "matching_score_pred = matching_score_pred / nb_sample\n",
    "\n",
    "fid = calculate_frechet_distance(gt_mu, gt_cov, mu, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1043da-b8ed-446e-9f66-a0120de4cae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd59e6d-fdac-4ba4-b8b9-5de6cc4a3a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30786dce-d468-4bde-9bf0-c58a012e3aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fb2fc23-05d4-43fc-9bec-577fe6bbdccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0757f-4335-4801-9499-cbb857722195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db1101-52a9-4a0e-b039-64c0f798e178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d0117-354e-4584-b3cb-7ce16d978317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb440614-6183-4471-b11d-ae8e13579beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ec39b-8e98-49e8-b8ff-643aaae11482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123bcc1-60c8-4bfd-8cfd-33279f434a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447f334-ea7d-471c-abba-08bfc8a1a50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3935987-e119-4301-a167-a1aed4d5d2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5221e39d-3fe7-4b1c-a8a6-48c2b561127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GI(split1 , split2):\n",
    "    total1 = sum(split1)\n",
    "    ip1 =  1 - sum([(i/total1)**2 for i in split1])\n",
    "    total2 = sum(split2)\n",
    "    ip2 =  1 - sum([(i/total2)**2 for i in split2])\n",
    "    # n1 = len(split1)\n",
    "    # n2 = len(split2)\n",
    "    nt = total1 + total2\n",
    "    print(ip1 , ip2)\n",
    "\n",
    "    return (total1/nt) * ip1 + (total2/nt) * ip2\n",
    "\n",
    "def CE(split1 , split2):\n",
    "    total1 = sum(split1)\n",
    "    ip1 =  1 - max([(i/total1) for i in split1])\n",
    "    total2 = sum(split2)\n",
    "    ip2 =  1 - max([(i/total2) for i in split2])\n",
    "    # n1 = len(split1)\n",
    "    # n2 = len(split2)\n",
    "    nt = total1 + total2\n",
    "    print(ip1 , ip2)\n",
    "\n",
    "    return (total1/nt) * ip1 + (total2/nt) * ip2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6bfaf80d-3863-4763-9366-b4d286540fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6325"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Work\n",
    "# GI([15 , 20 , 10] , [15 , 10 , 20] )\n",
    "# GI([20 , 10 , 20] , [10 , 20 , 10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "283e25d9-e520-4bff-961b-e49f216ced2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6419753086419753"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = [20 , 10 , 15]\n",
    "total = sum(split)\n",
    "1 - sum([(i/total)**2 for i in split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "627c81d8-97af-47fb-934b-bed07e7fd851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6419753086419753"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = [10 , 20 , 15]\n",
    "total = sum(split)\n",
    "1 - sum([(i/total)**2 for i in split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "85e6e508-7d27-4d3c-8418-7d0773144c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667 0.6666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE([30 , 30 , 30] , [30 , 30 , 30]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8ed03821-2688-43a6-96cb-36ffdf27178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE([30 , 15 , 15] , [0 , 15 , 15] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9a4e7fd8-995f-4a46-975e-b7e44fa5775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555555555555556 0.4444444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE([20 , 20 , 5] , [10 , 10 , 25] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cbe43416-f998-46cc-b3ea-9fe320ee2b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667 0.6666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GI([30 , 30 , 30] , [30 , 30 , 30]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "49e5c1d4-3d24-4276-ba48-7e9d004fbec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5833333333333333"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GI([30 , 15 , 15] , [0 , 15 , 15] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5171696b-f92e-4698-8e0b-fb6424e88b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5925925925925926 0.5925925925925926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5925925925925926"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GI([20 , 20 , 5] , [10 , 10 , 25] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "77b58bf2-c39e-4029-9737-7fc991143bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07440740740740748"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.667 - 0.5925925925925926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "139eb386-219a-414d-8ffb-9f5d1714ef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5925925925925926"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6d281dfb-980f-495f-b6d9-e8d11c03666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'momask-codes'...\n",
      "remote: Enumerating objects: 240, done.\u001b[K\n",
      "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
      "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
      "remote: Total 240 (delta 56), reused 43 (delta 43), pack-reused 162\u001b[K\n",
      "Receiving objects: 100% (240/240), 1.14 MiB | 17.48 MiB/s, done.\n",
      "Resolving deltas: 100% (111/111), done.\n",
      "Updating files: 100% (77/77), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/EricGuo5513/momask-codes.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4223f-ce0b-493d-9748-827dca1295ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
