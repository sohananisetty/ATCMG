{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8138b8-714c-4bd5-bc6b-97cc9afa7a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e9292e-e925-4197-9b6d-c67ec0534a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e3b77a-d030-46b2-b0cf-8908bb4890fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b4be21-bd73-456d-9c1e-a74aa6f2c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from functools import partial\n",
    "from torch import einsum, nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import pack, rearrange, reduce, repeat, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3eec76-3888-498f-9e2e-65946d093030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.text_encoders import T5Conditioner,MPNETConditioner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ceccd3-4224-4a0d-baf7-dfccffb3a4af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8975ceab-7c8f-4a1f-a2fe-d68257c91570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "def load_vqvae(gen_cfg):\n",
    "\n",
    "    body_cfg = get_cfg_defaults()\n",
    "    body_cfg.merge_from_file(gen_cfg.vqvae.body_config)\n",
    "    body_model = (\n",
    "        instantiate_from_config(body_cfg.vqvae).to(device).eval()\n",
    "    )\n",
    "    body_model.load(os.path.join(body_cfg.output_dir, \"vqvae_motion.pt\"))\n",
    "\n",
    "    if gen_cfg.vqvae.left_hand_config is  None and gen_cfg.vqvae.right_hand_config is None:\n",
    "        return body_model, body_cfg\n",
    "    \n",
    "    if gen_cfg.vqvae.left_hand_config is not None:\n",
    "        left_cfg = get_cfg_defaults()\n",
    "        left_cfg.merge_from_file(gen_cfg.vqvae.left_hand_config)\n",
    "        left_hand_model = instantiate_from_config(left_cfg.vqvae).to(device).eval()\n",
    "        left_hand_model.load(\n",
    "            os.path.join(left_cfg.output_dir, \"vqvae_motion.pt\")\n",
    "        )\n",
    "    else:\n",
    "        left_hand_model = None\n",
    "        \n",
    "    if gen_cfg.vqvae.right_hand_config is not None:\n",
    "        right_cfg = get_cfg_defaults()\n",
    "        right_cfg.merge_from_file(gen_cfg.vqvae.right_hand_config)\n",
    "        right_hand_model = instantiate_from_config(right_cfg.vqvae).to(device).eval()\n",
    "        right_hand_model.load(\n",
    "            os.path.join(right_cfg.output_dir, \"vqvae_motion.pt\")\n",
    "        )\n",
    "    else:\n",
    "        right_hand_model = None\n",
    "\n",
    "    return body_model, left_hand_model , right_hand_model , body_cfg , left_cfg , right_cfg\n",
    "\n",
    "def bkn_to_motion( codes, dset , remove_translation = True):\n",
    "    # codes b k n\n",
    "\n",
    "    k = codes.shape[1]\n",
    "    mrep = dset.motion_rep\n",
    "\n",
    "    if k == 1:\n",
    "        if mrep == MotionRep(\"body\"):\n",
    "\n",
    "            body_inds = codes[:, 0]\n",
    "            body_motion = body_model.decode(body_inds[0:1]).detach().cpu()\n",
    "\n",
    "            if remove_translation:\n",
    "                z = torch.zeros(\n",
    "                    body_motion.shape[:-1] + (2,),\n",
    "                    dtype=body_motion.dtype,\n",
    "                    device=body_motion.device,\n",
    "                )\n",
    "                body_motion = torch.cat(\n",
    "                    [body_motion[..., 0:1], z, body_motion[..., 1:]], -1\n",
    "                )\n",
    "\n",
    "            body_M = dset.toMotion(\n",
    "                body_motion[0],\n",
    "                motion_rep=MotionRep(\"body\"),\n",
    "                hml_rep=body_cfg.dataset.hml_rep,\n",
    "            )\n",
    "\n",
    "            return body_M\n",
    "\n",
    "        elif mrep == MotionRep(\"left_hand\"):\n",
    "\n",
    "            left_inds = codes[:, 0]\n",
    "            left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "            left_M = dset.toMotion(\n",
    "                left_motion[0],\n",
    "                motion_rep=MotionRep(left_cfg.dataset.motion_rep),\n",
    "                hml_rep=left_cfg.dataset.hml_rep,\n",
    "            )\n",
    "            return left_M\n",
    "\n",
    "        elif mrep == MotionRep(\"right_hand\"):\n",
    "            right_inds = codes[:, 0]\n",
    "            right_motion = (\n",
    "                right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "            )\n",
    "            right_M = dset.toMotion(\n",
    "                right_motion[0],\n",
    "                motion_rep=MotionRep(right_cfg.dataset.motion_rep),\n",
    "                hml_rep=right_cfg.dataset.hml_rep,\n",
    "            )\n",
    "            return right_M\n",
    "\n",
    "    if k == 2:\n",
    "        left_inds = codes[:, 0]\n",
    "        right_inds = codes[:, 1]\n",
    "\n",
    "        left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "        right_motion = right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "\n",
    "        left_M = dset.toMotion(\n",
    "            left_motion[0],\n",
    "            motion_rep=MotionRep(left_cfg.dataset.motion_rep),\n",
    "            hml_rep=left_cfg.dataset.hml_rep,\n",
    "        )\n",
    "        right_M = dset.toMotion(\n",
    "            right_motion[0],\n",
    "            motion_rep=MotionRep(right_cfg.dataset.motion_rep),\n",
    "            hml_rep=right_cfg.dataset.hml_rep,\n",
    "        )\n",
    "        hand_M = left_M + right_M\n",
    "        hand_M.motion_rep = MotionRep.HAND\n",
    "        hand_M.hml_rep = \"\".join(\n",
    "            [i for i in left_M.hml_rep if i in right_M.hml_rep]\n",
    "        )\n",
    "        return hand_M\n",
    "\n",
    "    if k == 3:\n",
    "        left_inds = codes[:, 1]\n",
    "        right_inds = codes[:, 2]\n",
    "        body_inds = codes[:, 0]\n",
    "        body_motion = body_model.decode(body_inds[0:1]).detach().cpu()\n",
    "\n",
    "        \n",
    "        if remove_translation:\n",
    "            z = torch.zeros(\n",
    "                body_motion.shape[:-1] + (2,),\n",
    "                dtype=body_motion.dtype,\n",
    "                device=body_motion.device,\n",
    "            )\n",
    "            body_motion = torch.cat([body_motion[..., 0:1], z, body_motion[..., 1:]], -1)\n",
    "\n",
    "        left_motion = left_hand_model.decode(left_inds[0:1]).detach().cpu()\n",
    "        right_motion = right_hand_model.decode(right_inds[0:1]).detach().cpu()\n",
    "\n",
    "        body_M = dset.toMotion(\n",
    "            body_motion[0],\n",
    "            motion_rep=MotionRep(\"body\"),\n",
    "            hml_rep = body_cfg.dataset.hml_rep)\n",
    "\n",
    "        left_M = dset.toMotion(\n",
    "            left_motion[0],\n",
    "            motion_rep=MotionRep(\"left_hand\"),\n",
    "            hml_rep=left_cfg.dataset.hml_rep)\n",
    "        right_M = dset.toMotion(\n",
    "            right_motion[0],\n",
    "            motion_rep=MotionRep(\"right_hand\"),\n",
    "            hml_rep=right_cfg.dataset.hml_rep)\n",
    "        full_M = dset.to_full_joint_representation(body_M, left_M, right_M)\n",
    "        return full_M\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e36755-6af2-494a-aeb6-284f142732e8",
   "metadata": {},
   "source": [
    "## TMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7967e07f-6d65-481c-a0ee-2937b7209891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n",
    "from core.datasets.tmr_dataset import TMRDataset, load_dataset, simple_collate\n",
    "from core import MotionRep, TextRep, AudioRep\n",
    "from core.datasets.conditioner import ConditionProvider,ConditionFuser\n",
    "from configs.config_tmr import get_cfg_defaults as get_cfg_defaults_tmr\n",
    "from core.models.TMR.tmr import TMR\n",
    "from core.models.TMR.tmr import get_score_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ad421-c9f5-46b8-9812-879ccac41067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6681f-8f41-4f4f-a942-c29baea7eabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4742e48-d708-4488-9394-94b7130673c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmr_cfg = get_cfg_defaults_tmr()\n",
    "tmr_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/tmr/tmr.yaml\")\n",
    "tmr_cfg.freeze()\n",
    "dataset_args = tmr_cfg.dataset\n",
    "tmr_parms = tmr_cfg.tmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b957f-4315-45de-ba0e-2870ddc07124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2f864f-15d4-4dcf-8b8d-da939b66c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tmr_parms.pop(\"target\")\n",
    "motion_encoder = instantiate_from_config(tmr_cfg.motion_encoder).to(device)\n",
    "text_encoder = instantiate_from_config(tmr_cfg.text_encoder).to(device)\n",
    "motion_decoder = instantiate_from_config(tmr_cfg.motion_decoder).to(device)\n",
    "tmr = TMR(motion_encoder , text_encoder , motion_decoder , lr = tmr_cfg.train.learning_rate, **tmr_parms).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21716048-3b60-4717-b5d5-f0880baccebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/tmr/tmr.pt\")\n",
    "tmr.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb7234-2882-47ab-bb11-5619e19c1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents(inputs , conditions):\n",
    "    text_conds = conditions[\"text\"]\n",
    "    text_x_dict = {\"x\": text_conds[0], \"mask\": text_conds[1].to(torch.bool)}\n",
    "    motion_x_dict = {\"x\": inputs[0], \"mask\": inputs[1].to(torch.bool)}\n",
    "    motion_mask = motion_x_dict[\"mask\"]\n",
    "    text_mask = text_x_dict[\"mask\"]\n",
    "    t_motions, t_latents, t_dists = tmr(\n",
    "            text_x_dict, mask=motion_mask, return_all=True\n",
    "        )\n",
    "\n",
    "    # motion -> motion\n",
    "    m_motions, m_latents, m_dists = tmr(\n",
    "        motion_x_dict, mask=motion_mask, return_all=True\n",
    "    )\n",
    "    return t_latents , m_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed256d-a248-4716-a5e8-9c6524a130d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f2f7c-8cb3-491a-b138-61a6e2c2e871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525e9e9-23b9-47d3-b876-fb8f6f9ab9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d6821a6-6b7f-443c-964f-aace12c3acec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions idea400: 577 and texts 577\n",
      "Total number of motions choreomaster: 2 and texts 2\n"
     ]
    }
   ],
   "source": [
    "train_ds, sampler_train, weights_train  = load_dataset(dataset_names = [\"idea400\" , \"choreomaster\"] , dataset_args=dataset_args, split = \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c0846-4bdd-4837-b273-f70a12df8631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c0425f1-f3a5-4759-bf91-efc58902bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            text_conditioner_name = dataset_args.text_conditioner_name,\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            motion_max_length_s=dataset_args.motion_max_length_s,\n",
    "            fps=30,\n",
    "            # only_motion = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa2b85e-c70e-4ba9-98f8-f71df0fd8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        4,\n",
    "        # sampler=sampler_train,\n",
    "        # shuffle = False,\n",
    "        collate_fn=partial(simple_collate , conditioner = condition_provider),\n",
    "        # drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a29728-a7fe-43e8-ab2b-63471be5bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, conditions in (train_loader):\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48def63b-97b2-4cb6-837d-d4296b1f6e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5a0c5-28cb-48ca-a99e-448313038f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "111553d7-8161-4d98-b73c-0eaa241ec81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = inputs[\"motion\"][0]\n",
    "motion_mask = inputs[\"motion\"][1].to(torch.bool)\n",
    "text = conditions[\"text\"][0]\n",
    "text_mask = conditions[\"text\"][1].to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0266a5ae-a146-45cb-9d00-fdcafd773c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tmr.compute_loss(inputs[\"motion\"] , conditions)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aae80dab-42b2-49ca-9745-2ab20ad99f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_latents , m_latents = get_latents(inputs[\"motion\"],conditions)\n",
    "score = get_score_matrix(t_latents, m_latents).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c1721d8-5d4b-422c-9324-4d126d3bfa57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6846c6-b958-45fe-8e75-08a717c15ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f216fca-1219-47ce-817f-5496e873e19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a806869-58a1-4ee3-9a13-1b5227bded8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4d8b0e6-ea5e-45f0-8ea3-5e04fa6452ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "630efeb2-be7c-4ad3-ac1e-5686e198cd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e16941-4ab9-4502-8c83-8b4fd929f757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ca228-3199-4c19-9007-fe08d2c3eba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79b775a9-8696-4a45-b850-cb13eb3a02be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34113e14-9675-4914-9443-b0bf5516c4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e46dbbc-b309-4f3a-94f0-79c70fa01102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bb6e1-be80-4ce4-b846-26793863e94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069dcc3e-dc06-4694-8a6c-759234874651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5249b-4c80-4762-b6fb-e661e6aa6a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d388de8-8d9b-46ae-85a0-80ce1c6b87a1",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e87b442-10d3-4d4b-b9d8-b6c330b7b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.TMR.tmr import get_score_matrix\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6368d74b-e83a-4201-ba79-aae08c3b82b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "condition_provider = ConditionProvider(\n",
    "            text_conditioner_name = dataset_args.text_conditioner_name,\n",
    "            motion_rep=MotionRep(dataset_args.motion_rep),\n",
    "            audio_rep=AudioRep(dataset_args.audio_rep),\n",
    "            text_rep=TextRep(dataset_args.text_rep),\n",
    "            motion_padding=dataset_args.motion_padding,\n",
    "            motion_max_length_s=dataset_args.motion_max_length_s,\n",
    "            fps=30,\n",
    "            # only_motion = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0db1e5-536a-4290-a3c2-f974481ea8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.base_dataset import BaseMotionDataset\n",
    "base_dset = BaseMotionDataset(motion_rep=MotionRep.FULL , hml_rep= \"gprvc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cd602886-407e-4085-a0ad-83859bbc8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(text:str , motion_path:str, return_embeds = False):\n",
    "    motion = np.load(motion_path)\n",
    "\n",
    "    text_embed, text_mask = condition_provider._get_text_features(\n",
    "        raw_text=text,\n",
    "    )\n",
    "    processed_motion = base_dset.get_processed_motion(\n",
    "            motion, motion_rep=MotionRep(dataset_args.motion_rep), hml_rep=dataset_args.hml_rep\n",
    "        )\n",
    "    mot_body = torch.Tensor(processed_motion()).to(device)[None]\n",
    "    print(mot_body.shape)\n",
    "    motion_x_dict = {\"x\": mot_body, \"mask\": torch.ones_like(mot_body)[...,0].to(torch.bool)}\n",
    "    text_x_dict = {\"x\": text_embed, \"mask\": text_mask.to(torch.bool)}\n",
    "    with torch.no_grad():\n",
    "        lat_m = tmr.encode(motion_x_dict, sample_mean=True)[0]\n",
    "        lat_t = tmr.encode(text_x_dict, sample_mean=True)[0]\n",
    "        score = get_score_matrix(lat_t, lat_m).cpu()\n",
    "\n",
    "    if return_embeds:\n",
    "        return score ,lat_m ,lat_t\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3ba3c-e207-499d-b2ef-ce489761561f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a86c1844-6823-4d4a-9de2-f323f830aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"a man sets to do a backflips then fails back flip and falls to the ground\"\n",
    "mot=\"/srv/hays-lab/scratch/sanisetty3/motionx/motion_data/new_joint_vecs/humanml/001034.npy\"\n",
    "text2 = \"a person standing loses balance falling to the right and recovers standing\"\n",
    "mot2=\"/srv/hays-lab/scratch/sanisetty3/motionx/motion_data/new_joint_vecs/humanml/000012.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a06df4a-9831-4f7e-be47-07ac7c272b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = np.load(mot)\n",
    "motion2 = np.load(mot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "347af130-3f12-4742-8aea-64fa378b7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 120, 137])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7139)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getScore(text , motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f3e94ca4-7c8e-4fea-947a-974956aad729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 120, 137])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9692)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getScore(text2 , motion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e72932-f9b1-45e6-958c-13a8b7ff6d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00413697-fd2b-48d8-93a3-14ac214c0053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ced5dfa-75f1-48fd-9268-737a055da2da",
   "metadata": {},
   "source": [
    "## Muse Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d21be686-931f-433d-a64c-1cd38813f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.generation.muse2 import generate_animation\n",
    "from core import MotionTokenizerParams, pattern_providers\n",
    "\n",
    "from core.param_dataclasses import pattern_providers\n",
    "from core.datasets.multimodal_dataset import MotionIndicesAudioTextDataset, load_dataset_gen, simple_collate\n",
    "from core.models.utils import instantiate_from_config, get_obj_from_str\n",
    "from core import MotionRep, AudioRep, TextRep\n",
    "from core.datasets.conditioner import ConditionProvider,ConditionFuser\n",
    "from core.models.generation.muse2 import MotionMuse as MotionMuse2\n",
    "import einops\n",
    "from configs.config_t2m import get_cfg_defaults as muse_get_cfg_defaults\n",
    "from core import MotionTokenizerParams\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "546a5812-7558-4d4e-b466-ead30e4811c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_cfg = muse_get_cfg_defaults()\n",
    "gen_cfg.merge_from_file(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_muse_body_hands/motion_muse_body_hands.yaml\")\n",
    "gen_cfg.freeze()\n",
    "tranformer_config = gen_cfg.motion_generator\n",
    "fuse_config = gen_cfg.fuser\n",
    "pattern_config = gen_cfg.codebooks_pattern\n",
    "dataset_args = gen_cfg.dataset\n",
    "\n",
    "target = tranformer_config.pop(\"target\")\n",
    "motion_gen = MotionMuse2(tranformer_config , fuse_config , pattern_config).to(device).eval()\n",
    "pkg = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_muse_body_hands/motion_muse.pt\", map_location=\"cuda\")\n",
    "motion_gen.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39ff8875-8aef-4813-8754-8a792ed45ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync is turned on False\n",
      "loaded model with  0.03015906736254692 tensor([110000.], device='cuda:0') steps\n"
     ]
    }
   ],
   "source": [
    "body_model, left_hand_model , right_hand_model , body_cfg , left_cfg , right_cfg = load_vqvae(gen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc341171-18e4-4a3a-b1e6-c0c7024e21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.base_dataset import BaseMotionDataset\n",
    "base_dset = BaseMotionDataset(motion_rep=MotionRep.BODY , hml_rep= \"gpvc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bfe6a-4f2e-4e2f-8c65-3048bdc59673",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_provider.audio_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd8d21-8cae-44b5-84d6-b959ee16a691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8537f1d-e1a7-479d-9821-83a8b5696c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(183.)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"lens\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b381ca6e-6a0b-43bf-957d-9b7a98a95915",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = inputs[\"texts\"][-1]\n",
    "duration_s = inputs[\"lens\"][-1]//30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "67ebac02-2f96-4874-9a8c-e670640a8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_s = int(inputs[\"lens\"].min()//30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8ba537a9-3817-4b6a-a362-ff98a92eb891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 36.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 37.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 37.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 37.02it/s]\n"
     ]
    }
   ],
   "source": [
    "all_ids_body = generate_animation(motion_gen , condition_provider ,overlap = 15, duration_s = duration_s ,  text = text_ , use_token_critic = True, timesteps = 24 )\n",
    "gen_motion = bkn_to_motion(all_ids_body[:,:1], base_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd22349-ae5f-46c3-8310-93eac89c54f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2faf152d-1acb-4483-b070-86dd0e1d95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_motion = bkn_to_motion(all_ids_body[:,:1], base_dset)\n",
    "# base_dset.render_hml(\n",
    "#                     gen_motion,\n",
    "#                     f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gen_novel_full.gif\",\n",
    "#                     zero_trans = True,\n",
    "#                     zero_orient = True,\n",
    "    \n",
    "#                 )\n",
    "# Image(open(f\"/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/render/gen_novel_full.gif\",'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db01795b-d4d4-4de9-8d7f-ec9437604c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 137])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_motion().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efcfcb-b03b-46d7-b6bc-4c9c90738398",
   "metadata": {},
   "source": [
    "### Eval script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391667b8-1106-45a7-8b8b-c0bcd54a617d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e60f5567-0757-4417-bec4-8399adee200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 299, 137])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"motion\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cd0f8c7f-950e-4a9a-9e4e-8a917b35b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 36.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 36.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 36.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 137])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (253) must match the existing size (240) at non-singleton dimension 1.  Target sizes: [1, 253, 137].  Tensor sizes: [240, 137]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     gen_motion \u001b[38;5;241m=\u001b[39m bkn_to_motion(all_ids_body[:,:\u001b[38;5;241m1\u001b[39m], base_dset)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(gen_motion()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mpred_pose_eval\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mlenn\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m gen_motion()[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     22\u001b[0m t_latents_pred , m_latents_pred \u001b[38;5;241m=\u001b[39m get_latents((pred_pose_eval , inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotion\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m] ), conditions)\n\u001b[1;32m     23\u001b[0m score_pred \u001b[38;5;241m=\u001b[39m get_score_matrix(t_latents_pred, m_latents_pred)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (253) must match the existing size (240) at non-singleton dimension 1.  Target sizes: [1, 253, 137].  Tensor sizes: [240, 137]"
     ]
    }
   ],
   "source": [
    "motion_list = []\n",
    "motion_pred_list = []\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "with torch.no_grad():\n",
    "    \n",
    "    t_latents , m_latents = get_latents(inputs[\"motion\"],conditions)\n",
    "    score = get_score_matrix(t_latents, m_latents).cpu()\n",
    "\n",
    "    pred_pose_eval = torch.zeros_like(inputs[\"motion\"][0])\n",
    "    for k in range(4):\n",
    "        lenn = int(inputs[\"lens\"][k])\n",
    "        text_ = inputs[\"texts\"][k]\n",
    "        duration_s = int(lenn//30)\n",
    "        all_ids_body = generate_animation(motion_gen , condition_provider ,overlap = 15, duration_s = duration_s ,  text = text_ , use_token_critic = True, timesteps = 24 )\n",
    "        gen_motion = bkn_to_motion(all_ids_body[:,:1], base_dset)\n",
    "        print(gen_motion().shape)\n",
    "        pred_pose_eval[k:k+1,:lenn] = gen_motion()[None]\n",
    "\n",
    "    t_latents_pred , m_latents_pred = get_latents((pred_pose_eval , inputs[\"motion\"][1] ), conditions)\n",
    "    score_pred = get_score_matrix(t_latents_pred, m_latents_pred).cpu()\n",
    "\n",
    "    motion_list.append(m_latents)\n",
    "    motion_pred_list.append(m_latents_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3d1da372-8e44-47f6-945d-f3dc7b3d0b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 137])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_motion().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "920a3276-27ab-4e0b-bfb0-d61892c293c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose_eval[0,:240] = gen_motion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd59e6d-fdac-4ba4-b8b9-5de6cc4a3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_R, temp_match = calculate_R_precision(\n",
    "    t_latents.cpu().numpy(), m_latents.cpu().numpy(), top_k=3, sum_all=True\n",
    ")\n",
    "R_precision_real += temp_R\n",
    "matching_score_real += temp_match\n",
    "temp_R, temp_match = calculate_R_precision(\n",
    "    t_latents_pred.cpu().numpy(), m_latents_pred.cpu().numpy(), top_k=3, sum_all=True\n",
    ")\n",
    "R_precision += temp_R\n",
    "matching_score_pred += temp_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30786dce-d468-4bde-9bf0-c58a012e3aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5221e39d-3fe7-4b1c-a8a6-48c2b561127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GI(split1 , split2):\n",
    "    total1 = sum(split1)\n",
    "    ip1 =  1 - sum([(i/total1)**2 for i in split1])\n",
    "    total2 = sum(split2)\n",
    "    ip2 =  1 - sum([(i/total2)**2 for i in split2])\n",
    "    # n1 = len(split1)\n",
    "    # n2 = len(split2)\n",
    "    nt = total1 + total2\n",
    "    print(ip1 , ip2)\n",
    "\n",
    "    return (total1/nt) * ip1 + (total2/nt) * ip2\n",
    "\n",
    "def CE(split1 , split2):\n",
    "    total1 = sum(split1)\n",
    "    ip1 =  1 - max([(i/total1) for i in split1])\n",
    "    total2 = sum(split2)\n",
    "    ip2 =  1 - max([(i/total2) for i in split2])\n",
    "    # n1 = len(split1)\n",
    "    # n2 = len(split2)\n",
    "    nt = total1 + total2\n",
    "    print(ip1 , ip2)\n",
    "\n",
    "    return (total1/nt) * ip1 + (total2/nt) * ip2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6bfaf80d-3863-4763-9366-b4d286540fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6325"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Work\n",
    "# GI([15 , 20 , 10] , [15 , 10 , 20] )\n",
    "# GI([20 , 10 , 20] , [10 , 20 , 10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "283e25d9-e520-4bff-961b-e49f216ced2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6419753086419753"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = [20 , 10 , 15]\n",
    "total = sum(split)\n",
    "1 - sum([(i/total)**2 for i in split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "627c81d8-97af-47fb-934b-bed07e7fd851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6419753086419753"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = [10 , 20 , 15]\n",
    "total = sum(split)\n",
    "1 - sum([(i/total)**2 for i in split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "85e6e508-7d27-4d3c-8418-7d0773144c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667 0.6666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE([30 , 30 , 30] , [30 , 30 , 30]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8ed03821-2688-43a6-96cb-36ffdf27178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE([30 , 15 , 15] , [0 , 15 , 15] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9a4e7fd8-995f-4a46-975e-b7e44fa5775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555555555555556 0.4444444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE([20 , 20 , 5] , [10 , 10 , 25] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cbe43416-f998-46cc-b3ea-9fe320ee2b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667 0.6666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GI([30 , 30 , 30] , [30 , 30 , 30]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "49e5c1d4-3d24-4276-ba48-7e9d004fbec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5833333333333333"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GI([30 , 15 , 15] , [0 , 15 , 15] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5171696b-f92e-4698-8e0b-fb6424e88b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5925925925925926 0.5925925925925926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5925925925925926"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GI([20 , 20 , 5] , [10 , 10 , 25] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "77b58bf2-c39e-4029-9737-7fc991143bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07440740740740748"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.667 - 0.5925925925925926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "139eb386-219a-414d-8ffb-9f5d1714ef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5925925925925926"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6d281dfb-980f-495f-b6d9-e8d11c03666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'momask-codes'...\n",
      "remote: Enumerating objects: 240, done.\u001b[K\n",
      "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
      "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
      "remote: Total 240 (delta 56), reused 43 (delta 43), pack-reused 162\u001b[K\n",
      "Receiving objects: 100% (240/240), 1.14 MiB | 17.48 MiB/s, done.\n",
      "Resolving deltas: 100% (111/111), done.\n",
      "Updating files: 100% (77/77), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/EricGuo5513/momask-codes.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4223f-ce0b-493d-9748-827dca1295ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
