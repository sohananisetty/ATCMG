model_name: "motion_muse_constant"
output_dir: "/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/motion_muse_constant"
dataset:
  dataset_name: "mix"
  dataset_root: "/srv/hays-lab/scratch/sanisetty3/motionx"
  motion_rep: "body"
  text_conditioner_name: "openai/clip-vit-large-patch14" #"laion/larger_clap_music_and_speech"
  text_rep: "pooled_text_embed"
  audio_rep: "encodec"
  hml_rep: "rv"
  motion_min_length_s: 4
  motion_max_length_s: 4
  window_size_s: 4
train:
  resume: False
  num_train_iters : 110001 #'Number of training steps
  save_steps : 10000
  logging_steps : 10
  wandb_every : 200
  evaluate_every : 10000
  eval_bs : 400
  train_bs : 400
  gradient_accumulation_steps : 1
  learning_rate : 1e-4
  weight_decay : 1e-3
  warmup_steps : 4000
  gamma : 0.05
  lr_scheduler_type : "cosine"

vqvae:
  body_config: "/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_rv/vqvae_rv.yaml"
  left_hand_config: "/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_left_hand/vqvae_left_hand.yaml"
  right_hand_config: "/srv/hays-lab/scratch/sanisetty3/music_motion/ATCMG/checkpoints/vqvae/vqvae_right_hand/vqvae_right_hand.yaml"

codebooks_pattern:
  modeling: "parallel"

fuser:
  fuse_method: [{"cross": ["text"], "input_interpolate": ["audio"]}]
  #[{"cross": ["audio"], "prepend": ["text"]}]

motion_generator:
  target: "core.models.generation.motion_generator.MotionMuse"
  n_q : 1
  dim : 512
  depth: 12
  cond_dropout: 0.4
  no_mask_token_prob: 0.4
  audio_input_dim: 128
  text_input_dim: 768
  emb_dropout: 0.1
  num_tokens: 512
  flash: True
  var_len: False
  quality_emb: False
  custom: False
  attn_dropout: 0.1
  



  
  

